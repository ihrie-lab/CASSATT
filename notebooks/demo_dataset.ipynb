{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 206 µs (started: 2022-08-11 15:19:05 -05:00)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from skimage import io, transform, util, img_as_float\n",
    "from skimage.filters import gaussian\n",
    "from skimage.color import separate_stains, hax_from_rgb, rgb2gray\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.util import img_as_ubyte, crop\n",
    "from skimage.draw import rectangle, polygon\n",
    "from skimage.morphology import remove_small_objects, remove_small_holes\n",
    "from skimage.registration import phase_cross_correlation\n",
    "from skimage.measure import regionprops_table\n",
    "from skimage.segmentation import expand_labels, find_boundaries\n",
    "from skimage import metrics                                          \n",
    "from scipy.stats import entropy                                       \n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import napari\n",
    "import time\n",
    "import re\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "available_cores = multiprocessing.cpu_count()\n",
    "\n",
    "from stardist.models import StarDist2D\n",
    "from stardist import random_label_cmap\n",
    "from csbdeep.utils import normalize\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 490 µs (started: 2022-08-11 10:48:53 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Set dev_mode to True to print step by step outputs to confirm pipeline is working as expected and find and correct issues\n",
    "dev_mode = True\n",
    "# Set batch_mode to False to run workflow on a single slide - ideal for setting up workflow for the first time. Set to True to run multiple slides at the same time.\n",
    "batch_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 932 µs (started: 2022-08-11 15:19:12 -05:00)\n"
     ]
    }
   ],
   "source": [
    "#set pipeline output directory\n",
    "output_dir = None\n",
    "#set image input directory\n",
    "raw_img_dir = None  # for each round of imaging, images should be saved by the slide name and placed in a folder named for the marker that has been stained in the contained images\n",
    "# input order of MxIHC staining\n",
    "stain_order = ['PD-1', 'PD-L1', 'CD68', 'CD3', 'FoxP3', 'Iba-1', 'CD8', 'CD4']   # order in which antibodies were stained\n",
    "# input patient IDs\n",
    "slide_id = ['N16-244-2A']   \n",
    "# Stardist Model Directory\n",
    "stardist_dir = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.65 ms (started: 2022-08-11 15:19:12 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Setting variables\n",
    "\n",
    "#BaseAligned image input\n",
    "input_folder = output_dir + '/BaseAligned'\n",
    "#Tiles output\n",
    "output_folder = output_dir + '/Tiled'\n",
    "# registered stack output\n",
    "reg_folder = output_dir + '/Registered'\n",
    "# cropped stack output\n",
    "crops_folder = output_dir + '/CroppedStacks'\n",
    "# reassembled registered slides output\n",
    "stitch_folder = output_dir + '/StitchedSlides'\n",
    "# nuclear labels output if using built in Stardist, input if importing segmentation\n",
    "labels_folder = output_dir + '/Labels'\n",
    "# plot outputs folder\n",
    "plots_folder = output_dir + '/Plots'\n",
    "\n",
    "tile_x, tile_y = 2048, 2048              # Can Edit Tile Size Here\n",
    "overlap_x, overlap_y = 512, 512          # Can Edit Overlap Here\n",
    "\n",
    "tissue_frac = 0.10                       # Fraction of tile area that must contain tissue to save tile (0-1)\n",
    "background_int = 250                     # Can Edit Mean Pixel Intensity for Tissue Detection Here (Sets mean pixel intensity value at which tiles are either discarded are saved during image tiling - saved blocks are noted with blue text in the tilemaps while discarded blocks are noted with red text)\n",
    "                                         # If tiles containing substantial tissue are discarded, adjust value higher, if tiles with no tissue are saved, adjust value lower\n",
    "\n",
    "_ransacReprojThreshold = 3               # Can Edit Maximum allowed reprojection error to treat a point pair as an inlier (used in the RANSAC method only) for Keypoint registration\n",
    "\n",
    "_num_good = 10                           # Can Edit Minimum number of good matches needed for keypoint registration to be considered successful (0 - 200)\n",
    "_averageDistance = 0.5                   # Can Edit Maximum average distance of keypoint matches for registration to be considered successful\n",
    "\n",
    "cell_reg_thresh = 0.97                   # Can Edit float value for tissue detection within cell registration (0-1). Tweaks minimum color difference from white to detect tissue             ### UPDATED\n",
    "tissue_loss_thresh = 75                  # Can Edit threshold for percent acceptable tissue loss per tile.                      ### UPDATED\n",
    "                                         # eg. if tissue_loss_threshold = 75, if 76% of a tissue in a tile is lost in the final round compared to round 0, tile is eliminated from analysis)     ### UPDATED\n",
    "stitch_all = True                       # Set to True if fully stitched and reassembled image stack is desired                 ### UPDATED\n",
    "stardist_segmentation = True             # Set to True if using stardist segmentation model, False if labels files will be generated and uploaded separately \n",
    "\n",
    "seg_channel = 4                          # Set to stain_order index with best hematoxylin staining              #### UPDATED\n",
    "\n",
    "exp_lab_dist = 10                        # Can Edit pixel distance to expand nuclear labels to capture cytoplasmic signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define Functions #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.8 ms (started: 2022-08-11 15:19:14 -05:00)\n"
     ]
    }
   ],
   "source": [
    "def closest(lst, K):    \n",
    "    return lst.index(lst[min(range(len(lst)), key = lambda i: abs(lst[i]-K))])\n",
    "\n",
    "\n",
    "def goalRes(file):\n",
    "    l_res = []\n",
    "    for index, page in enumerate(tifffile.TiffFile(file).pages):\n",
    "        try:\n",
    "            l_res.append([index, page.tags['XResolution'].value[1]])\n",
    "        except:\n",
    "            l_res.append([index, 0])    # catches pages where XResolution is not a valid tag\n",
    "    return closest([x[1] for x in l_res], 237000)   # this value is currently set specifically to return the 20X resolution image from specific SCN file type we get from DHSR\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.79 ms (started: 2022-08-11 15:19:14 -05:00)\n"
     ]
    }
   ],
   "source": [
    "def normalized_mutual_information(image0, image1, *, bins=100):                                            # UPDATED\n",
    "    if image0.ndim != image1.ndim:\n",
    "        raise ValueError(f'NMI requires images of same number of dimensions. '\n",
    "                         f'Got {image0.ndim}D for `image0` and '\n",
    "                         f'{image1.ndim}D for `image1`.')\n",
    "    if image0.shape != image1.shape:\n",
    "        max_shape = np.maximum(image0.shape, image1.shape)\n",
    "        padded0 = _pad_to(image0, max_shape)\n",
    "        padded1 = _pad_to(image1, max_shape)\n",
    "    else:\n",
    "        padded0, padded1 = image0, image1\n",
    "\n",
    "    hist, bin_edges = np.histogramdd(\n",
    "            [np.reshape(padded0, -1), np.reshape(padded1, -1)],\n",
    "            bins=bins,\n",
    "            density=True,\n",
    "            )\n",
    "\n",
    "    H0 = entropy(np.sum(hist, axis=0))\n",
    "    H1 = entropy(np.sum(hist, axis=1))\n",
    "    H01 = entropy(np.reshape(hist, -1))\n",
    "\n",
    "    return (H0 + H1) / H01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.1 ms (started: 2022-08-11 15:19:14 -05:00)\n"
     ]
    }
   ],
   "source": [
    "def deconvReg(image):\n",
    "    if type(image) == str:\n",
    "        img = io.imread(image)\n",
    "        hax  = separate_stains(img, hax_from_rgb)\n",
    "        hema = hax[:,:,0]\n",
    "        h = rescale_intensity(hema, in_range = (0, 0.1), out_range = 'uint8')  # in_range set to 0.1 here to get a stronger hema signal for better registration\n",
    "    elif type(image) == np.ndarray:\n",
    "        hax = separate_stains(image, hax_from_rgb)\n",
    "        hema = hax[:,:,0]\n",
    "        h = rescale_intensity(hema, in_range = (0, 0.1), out_range = 'uint8')\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.37 ms (started: 2022-08-11 15:19:15 -05:00)\n"
     ]
    }
   ],
   "source": [
    "def base_align(base):\n",
    "    l_sizes =  []\n",
    "    test_img = io.imread(base[0])\n",
    "    for n in range(8):\n",
    "        l_sizes.append(int(test_img.shape[0]/2**n) * int(test_img.shape[1]/2**n))\n",
    "    scale_down = 2**closest(l_sizes, 3000000)\n",
    "    del test_img\n",
    "    \n",
    "    l_sim = []\n",
    "    if dev_mode == True:\n",
    "        f = open('{}/{}_distance.txt'.format(input_folder, os.path.splitext(os.path.basename(base[0]))[0]), 'w+')  \n",
    "    os.chdir(output_dir)\n",
    "    original_base_0 = io.imread(base[0])\n",
    "    tifffile.imwrite(output_dir + '/BaseAligned/Aligned_{}'.format(os.path.basename(base[0])), original_base_0)    # saves fixed image (untransformed)\n",
    "    base_0_gray = img_as_ubyte(rgb2gray(cv2.resize(original_base_0, (int(original_base_0.shape[1]/scale_down), int(original_base_0.shape[0]/scale_down)))))\n",
    "\n",
    "\n",
    "    for index, x in enumerate(base[1:]):\n",
    "        original_moving = io.imread(x)\n",
    "        moving_gray = img_as_ubyte(rgb2gray(cv2.resize(original_moving, (int(original_moving.shape[1]/scale_down), int(original_moving.shape[0]/scale_down)))))\n",
    "\n",
    "        \n",
    "        fd = cv2.KAZE_create(extended=True)\n",
    "        try:\n",
    "            moving_pts, target_pts, averageDistance, num_good = match_keypoints(moving_gray, base_0_gray, feature_detector=fd)\n",
    "            \n",
    "            transformer = transform.EuclideanTransform()\n",
    "            try:\n",
    "                transformer.estimate(target_pts * scale_down, moving_pts * scale_down)\n",
    "                output_shape_rc = original_base_0.shape[:2]\n",
    "                warped_img = transform.warp(original_moving, transformer, output_shape=output_shape_rc)\n",
    "            except:\n",
    "                if dev_mode == True:\n",
    "                    f.write('registration failure on ' + str(index+1) + ' apply_transform')\n",
    "                    f.close()\n",
    "                break\n",
    "            warped_img = img_as_ubyte(warped_img)\n",
    "            tifffile.imwrite(output_dir + '/BaseAligned/Aligned_{}'.format(os.path.basename(x)), warped_img)  # save transformed image    \n",
    "            warped_gray = img_as_ubyte(rgb2gray(cv2.resize(warped_img, (int(warped_img.shape[1]/scale_down), int(warped_img.shape[0]/scale_down)))))\n",
    "            if dev_mode == True:                                                                              # UPDATED TO END\n",
    "                gaus_base = gaussian(base_0_gray, 2)\n",
    "                gaus_warped = gaussian(warped_gray, 2)\n",
    "                gaus_moving = gaussian(moving_gray, 2)\n",
    "                X = metrics.structural_similarity(gaus_base, gaus_warped)\n",
    "                Y = metrics.structural_similarity(gaus_base, gaus_moving)\n",
    "                X1 = normalized_mutual_information(gaus_base, gaus_warped, bins = 100)\n",
    "                Y1 = normalized_mutual_information(gaus_base, gaus_moving, bins = 100)\n",
    "\n",
    "                l_sim.append([os.path.basename(x), X, Y, X1, Y1])\n",
    "\n",
    "            if dev_mode == True:\n",
    "                f.write('{}_Average Distance {} = {}________Number of Good Matches = {}\\n'.format(str(index+1),str(averageDistance),os.path.basename(x) ,str(num_good)))\n",
    "        except:\n",
    "            if dev_mode == True:\n",
    "                f.write('registration failure on ' + str(index+1) + ' match_keypoints.\\n')\n",
    "    if dev_mode == True:\n",
    "        df = pd.DataFrame(l_sim, columns = ['moving_file', 'warped_sim', 'moving_sim', 'warped_nmi', 'moving_nmi'])\n",
    "        df.to_csv(output_dir + '/BaseAligned/Similarity_{}.csv'.format(os.path.splitext(os.path.basename(x))[0]), index = False)\n",
    "        f.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.67 ms (started: 2022-08-11 15:19:15 -05:00)\n"
     ]
    }
   ],
   "source": [
    "def tileSave(file):\n",
    "    tissue_detect = []\n",
    "    image = io.imread(file)\n",
    "    tile_rows_overlap = list(range(int(image.shape[0]/tile_x)))\n",
    "    tile_cols_overlap = list(range(int(image.shape[1]/tile_y)))\n",
    "    combined = [(f,s) for f in tile_rows_overlap for s in tile_cols_overlap]    \n",
    "    filename = os.path.basename(os.path.splitext(file)[0])   \n",
    "    _, _, _, pat, stain = filename.split('_')\n",
    "\n",
    "    os.makedirs('{}/{}'.format(output_folder, filename), exist_ok = True)\n",
    "    os.chdir('{}/{}'.format(output_folder,filename))\n",
    "\n",
    "\n",
    "    log = open('savelog.txt', 'w+')\n",
    "\n",
    "    # Create low res slide image with tile rows and columns labeled\n",
    "\n",
    "    imgScale = 0.25   # scale factor for low res slide image\n",
    "    newX, newY = int(image.shape[1]*imgScale), int(image.shape[0]*imgScale)\n",
    "    lowres = cv2.resize(image, (newX, newY))\n",
    "\n",
    "     # Save cropped tiles\n",
    "\n",
    "    for i, j in combined:   # cycle through tiles and \n",
    "        row_top_offset = max(0, i * tile_x - overlap_x)\n",
    "        row_bot_offset = max(0, image.shape[0] - (tile_x * (i+1) + overlap_x))\n",
    "        col_l_offset = max(0, j * tile_y - overlap_y)\n",
    "        col_r_offset = max(0, image.shape[1] - (tile_y * (j+1) + overlap_y))                                  ### UPDATED _ Tissue detection dropped here\n",
    "        croppedimg = crop(image,((row_top_offset,row_bot_offset),(col_l_offset, col_r_offset),(0,0)), copy = False)   \n",
    "        TEXT = '({}, {})'.format(i, j)\n",
    "        TEXT_SCALE = 3                                                # font sizes may need to be adjusted if input image size change\n",
    "        TEXT_THICKNESS = 3\n",
    "        TEXT_FACE = 3\n",
    "        text_size, _ = cv2.getTextSize(TEXT, TEXT_FACE, TEXT_SCALE, TEXT_THICKNESS)\n",
    "        LOC = (int(j*tile_x*imgScale + (tile_x*imgScale)/2), int(i*tile_y*imgScale + (tile_y*imgScale)/2))\n",
    "        TEXT_ORG = (int(LOC[0]-text_size[0]/2), int(LOC[1] + text_size[1]/2))        \n",
    "        alpha = 150                                                                                            ### UPDATED alpha value\n",
    "        out_thick = 5\n",
    "        color = (alpha, alpha, alpha)\n",
    "        TL = (int(i*tile_x*imgScale), int(j*tile_x*imgScale))\n",
    "        TR = (int(i*tile_x*imgScale), int((j+1)*tile_x*imgScale))\n",
    "        BR = (int((i+1)*tile_x*imgScale), int((j+1)*tile_x*imgScale))\n",
    "        BL = (int((i+1)*tile_x*imgScale), int(j*tile_x*imgScale))\n",
    "\n",
    "        rr, cc = rectangle(start = TL, end =(TR[0] + out_thick, TR[1]), shape = lowres.shape)\n",
    "        lowres[rr,cc] = color\n",
    "        rr, cc = rectangle(start = TR, end = (BR[0], BR[1]-out_thick), shape = lowres.shape)\n",
    "        lowres[rr,cc] = color\n",
    "        rr, cc = rectangle(start = BL, end = (BR[0] - out_thick, BR[1]), shape = lowres.shape)\n",
    "        lowres[rr,cc] = color\n",
    "        rr,cc = rectangle(start = TL, end = (BL[0], BL[1] + out_thick), shape = lowres.shape)\n",
    "        lowres[rr,cc] = color\n",
    "        cv2.putText(lowres, text = TEXT, org = TEXT_ORG, fontFace = TEXT_FACE, fontScale = TEXT_SCALE, color = color, thickness = TEXT_THICKNESS)     \n",
    "    for i, j in combined:\n",
    "        row_top_offset = max(0, i * tile_x - overlap_x)\n",
    "        row_bot_offset = max(0, image.shape[0] - (tile_x * (i+1) + overlap_x))\n",
    "        col_l_offset = max(0, j * tile_y - overlap_y)\n",
    "        col_r_offset = max(0, image.shape[1] - (tile_y * (j+1) + overlap_y))  \n",
    "        croppedimg = crop(image,((row_top_offset,row_bot_offset),(col_l_offset, col_r_offset),(0,0)), copy = False)   \n",
    "        #tissue detection block\n",
    "        croppedimg[np.where((croppedimg==[0,0,0]).all(axis=2))] = [255,255,255]   # Turns black pixels white          ### UPDATED below to line \"TEXT = ....\"\n",
    "        thresh = cell_reg_thresh\n",
    "        gaus = gaussian(croppedimg, 6, multichannel = True)\n",
    "        mask_gaus = ((gaus[:,:,0] > thresh) | (gaus[:,:,1] > thresh) | (gaus[:,:,2] > thresh)) \n",
    "        tissue_frac_tile = (np.size(mask_gaus)-np.count_nonzero(mask_gaus))/np.size(mask_gaus)     ## add tissue_frac to global variables\n",
    "        tissue_detect.append([i, j, tissue_frac_tile])\n",
    "        \n",
    "        TEXT = '({}, {})'.format(i, j)\n",
    "        TEXT_SCALE = 3                                                # font sizes may need to be adjusted if input image size change\n",
    "        TEXT_THICKNESS = 3\n",
    "        TEXT_FACE = 3\n",
    "        text_size, _ = cv2.getTextSize(TEXT, TEXT_FACE, TEXT_SCALE, TEXT_THICKNESS)\n",
    "        LOC = (int(j*tile_x*imgScale + (tile_x*imgScale)/2), int(i*tile_y*imgScale + (tile_y*imgScale)/2))\n",
    "        TEXT_ORG = (int(LOC[0]-text_size[0]/2), int(LOC[1] + text_size[1]/2))        \n",
    "\n",
    "        if tissue_frac_tile > tissue_frac: \n",
    "            io.imsave('{}_{}_r{}_c{}.tiff'.format(pat, stain, tile_rows_overlap[i], tile_cols_overlap[j]), croppedimg)\n",
    "            cv2.putText(lowres, text = TEXT, org = TEXT_ORG, fontFace = TEXT_FACE, fontScale = TEXT_SCALE, color = (0,0,0), thickness = TEXT_THICKNESS)  \n",
    "            alpha = 0\n",
    "            out_thick = 5\n",
    "            color = (alpha, alpha, alpha)\n",
    "            TL = (int(i*tile_x*imgScale), int(j*tile_x*imgScale))\n",
    "            TR = (int(i*tile_x*imgScale), int((j+1)*tile_x*imgScale))\n",
    "            BR = (int((i+1)*tile_x*imgScale), int((j+1)*tile_x*imgScale))\n",
    "            BL = (int((i+1)*tile_x*imgScale), int(j*tile_x*imgScale))\n",
    "\n",
    "            rr, cc = rectangle(start = (TL[0]-out_thick, TL[1]-out_thick), end =(TR[0] + out_thick, TR[1]-out_thick), shape = lowres.shape)\n",
    "            lowres[rr,cc] = color\n",
    "            rr, cc = rectangle(start = (TR[0]+out_thick, TR[1]-out_thick), end = (BR[0]+out_thick, BR[1]+out_thick), shape = lowres.shape)\n",
    "            lowres[rr,cc] = color\n",
    "            rr, cc = rectangle(start = (BL[0]-out_thick, BL[1]+out_thick), end = (BR[0] + out_thick, BR[1]+out_thick), shape = lowres.shape)\n",
    "            lowres[rr,cc] = color\n",
    "            rr,cc = rectangle(start = (TL[0]-out_thick, TL[1]-out_thick), end = (BL[0]-out_thick, BL[1] + out_thick), shape = lowres.shape)\n",
    "            lowres[rr,cc] = color  \n",
    "            \n",
    "        else:                                                                 ### UPDATED TO BELOW\n",
    "            TL = (int(i*tile_x*imgScale), int(j*tile_x*imgScale))\n",
    "            TR = (int(i*tile_x*imgScale), int((j+1)*tile_x*imgScale))\n",
    "            BR = (int((i+1)*tile_x*imgScale), int((j+1)*tile_x*imgScale))\n",
    "            BL = (int((i+1)*tile_x*imgScale), int(j*tile_x*imgScale))\n",
    "            alpha = 150\n",
    "\n",
    "            color = (alpha, alpha, alpha)\n",
    "            r = np.array([TL[0]+out_thick, TL[0], BR[0]-out_thick, BR[0]])\n",
    "            c = np.array([TL[1], TL[1] + out_thick, BR[1], BR[1]-out_thick])\n",
    "            rr, cc = polygon(r, c)\n",
    "            lowres[rr,cc] = color                                              ###UPDATED TO ABOVE\n",
    "    io.imsave('{}/{}_{}_tilemap.tiff'.format(output_folder, pat, stain), lowres)\n",
    "    df_tissue_detect = pd.DataFrame(tissue_detect, columns = ['row', 'col', 'tissue_frac'])    ## UPDATED\n",
    "    df_tissue_detect.to_csv('{}/{}_{}_tissue_detect.csv'.format(output_folder, pat, stain), index = False)\n",
    "    log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.09 ms (started: 2022-08-11 15:19:15 -05:00)\n"
     ]
    }
   ],
   "source": [
    "def match_keypoints(moving, target, feature_detector):\n",
    "\n",
    "    kp1, desc1 = feature_detector.detectAndCompute(moving, None)\n",
    "    kp2, desc2 = feature_detector.detectAndCompute(target, None)\n",
    "\n",
    "    matcher = cv2.BFMatcher(normType=cv2.NORM_L2, crossCheck=True)\n",
    "    matches = matcher.match(desc1, desc2)\n",
    "    matches_sorted = sorted(matches, key = lambda x: x.distance)\n",
    "    \n",
    "    totalDistance = 0\n",
    "    for g in matches_sorted:\n",
    "        totalDistance += g.distance\n",
    "        \n",
    "    if len(matches_sorted) == 0:\n",
    "        averageDistance = 0\n",
    "    else:\n",
    "        averageDistance = totalDistance/len(matches_sorted)\n",
    "\n",
    "\n",
    "    src_match_idx = [m.queryIdx for m in matches_sorted[:200]]   # list only first 200 matches can edit to select more or less\n",
    "    dst_match_idx = [m.trainIdx for m in matches_sorted[:200]]\n",
    "\n",
    "    src_points = np.float32([kp1[i].pt for i in src_match_idx])\n",
    "    dst_points = np.float32([kp2[i].pt for i in dst_match_idx])\n",
    "\n",
    "    H, mask = cv2.findHomography(src_points, dst_points, cv2.RANSAC, ransacReprojThreshold=_ransacReprojThreshold)   # original value = 7 \n",
    "\n",
    "    good = [matches_sorted[i] for i in np.arange(0, len(mask)) if mask[i] == [1]]\n",
    "    \n",
    "    num_good = len(good) # count how many good matches were found\n",
    "\n",
    "    filtered_src_match_idx = [m.queryIdx for m in good]\n",
    "    filtered_dst_match_idx = [m.trainIdx for m in good]\n",
    "\n",
    "    filtered_src_points = np.float32([kp1[i].pt for i in filtered_src_match_idx])\n",
    "    filtered_dst_points = np.float32([kp2[i].pt for i in filtered_dst_match_idx])\n",
    "\n",
    "    return filtered_src_points, filtered_dst_points, averageDistance, num_good\n",
    "\n",
    "def apply_transform(moving, target, moving_pts, target_pts, transformer, output_shape_rc=None):\n",
    "\n",
    "    if output_shape_rc is None:\n",
    "        output_shape_rc = target.shape[:2]\n",
    "\n",
    "    if str(transformer.__class__) == \"<class 'skimage.transform.EuclideanTransform'>\":\n",
    "        transformer.estimate(target_pts, moving_pts)\n",
    "        warped_img = transform.warp(moving, transformer, output_shape=output_shape_rc)\n",
    "\n",
    "        ### Restimate to warp points\n",
    "        transformer.estimate(moving_pts, target_pts)\n",
    "        warped_pts = transformer(moving_pts)\n",
    "    else:\n",
    "        transformer.estimate(moving_pts, target_pts)\n",
    "        warped_img = transform.warp(moving, transformer.inverse, output_shape=output_shape_rc)\n",
    "        warped_pts = transformer(moving_pts)\n",
    "\n",
    "    return warped_img, warped_pts\n",
    "\n",
    "def keypoint_distance(moving_pts, target_pts, img_h, img_w):\n",
    "    dst = np.sqrt(np.sum((moving_pts - target_pts)**2, axis=1)) / np.sqrt(img_h**2 + img_w**2)\n",
    "    return np.mean(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.8 ms (started: 2022-08-11 10:13:40 -05:00)\n"
     ]
    }
   ],
   "source": [
    "def regAll(path):\n",
    "    os.makedirs('{}/{}'.format(\n",
    "        reg_folder,\n",
    "        os.path.basename(path)), exist_ok = True)\n",
    "    os.chdir('{}/{}'.format(\n",
    "        reg_folder,\n",
    "        os.path.basename(path)))\n",
    "    if dev_mode == True:\n",
    "        f = open('{}/{}/distance.txt'.format(\n",
    "                reg_folder,\n",
    "                os.path.basename(path)), 'w+')   \n",
    "    \n",
    "    d_reg = {}\n",
    "    for i, k in enumerate(all_paths[path]):      \n",
    "        # block for registering image to stain_order0\n",
    "\n",
    "        original_target = io.imread(all_paths[path][0])  # rgb input\n",
    "        original_moving = io.imread(k)  # rgb\n",
    "\n",
    "        target_file = deconvReg(all_paths[path][0])   # Hema only\n",
    "        moving_file = deconvReg(k)   # Hema only\n",
    "\n",
    "        target = img_as_ubyte(gaussian(target_file, 3))    #blur on hema\n",
    "        moving = img_as_ubyte(gaussian(moving_file, 3))    #blur on hema\n",
    "                                                           #gaussian alpha can be tuned here\n",
    "        _, stainord, _, slide, stain = os.path.basename(os.path.dirname(k)).split('_')\n",
    "        _, _, row, col = os.path.basename(os.path.splitext(k)[0]).split('_')\n",
    "\n",
    "        fd = cv2.KAZE_create(extended=True)\n",
    "        try:\n",
    "            moving_pts, target_pts, averageDistance, num_good = match_keypoints(moving, target, feature_detector=fd)\n",
    "        except:\n",
    "            if dev_mode == True:\n",
    "                f.write('registration failure on' + str(i) + 'match_keypoints')\n",
    "                f.close()\n",
    "            break\n",
    "        if dev_mode == True:\n",
    "            f.write('{}_Average Distance {} = {}________Number of Good Matches = {}\\n'.format(str(i),str(averageDistance), all_paths[path][i],str(num_good)))\n",
    "\n",
    "        transformer = transform.EuclideanTransform()\n",
    "        try:\n",
    "            warped_img, warped_pts = apply_transform(original_moving, original_target, moving_pts, target_pts, transformer=transformer)\n",
    "        except:\n",
    "            if dev_mode == True:\n",
    "                f.write('registration failure on' + str(i) + 'apply_transform')\n",
    "                f.close()\n",
    "            break\n",
    "\n",
    "        warped_img = img_as_ubyte(warped_img)\n",
    "        d_reg[int(stainord),'i0'] = warped_img \n",
    "        if dev_mode == True:\n",
    "            io.imsave(str(i) + '_i0_' + (os.path.basename(os.path.splitext(all_paths[path][i])[0])+ '_reg.tiff'), warped_img)\n",
    "\n",
    "        \n",
    "        \n",
    "        # if a suboptimal registration is detected via low number of good keypoint matches or large average distance between matches\n",
    "        # instead of registering to the index0 image, attemp to register to the index -1 image (previous image in registration stack)\n",
    "        if num_good < _num_good or averageDistance > _averageDistance:  \n",
    "            \n",
    "            if (int(stainord)-1, 'i-1') in d_reg.keys():\n",
    "                original_target = d_reg[int(stainord)-1, 'i-1']\n",
    "            else:\n",
    "                original_target = d_reg[int(stainord)-1, 'i0']\n",
    "                \n",
    "            if (int(stainord)-1, 'i-1') in d_reg.keys():\n",
    "                target_file = deconvReg(d_reg[int(stainord)-1, 'i-1'])\n",
    "            else:\n",
    "                target_file = deconvReg(d_reg[int(stainord)-1, 'i0'])\n",
    "\n",
    "\n",
    "            target = img_as_ubyte(gaussian(target_file, 6))    #alpha could be tuned for best performace \n",
    "            moving = img_as_ubyte(gaussian(moving_file, 6))\n",
    "\n",
    "\n",
    "            fd = cv2.KAZE_create(extended=True)\n",
    "            try:\n",
    "                moving_pts, target_pts, averageDistance, num_good = match_keypoints(moving, target, feature_detector=fd)\n",
    "            except:\n",
    "                if dev_mode == True:\n",
    "                    f.write('fail on' + str(i) + '-1 match_keypoints')\n",
    "                    f.close()\n",
    "                break\n",
    "            if dev_mode == True:\n",
    "                f.write('{}_i-1_Average Distance {} = {}________Number of Good Matches = {}\\n'.format(str(i),str(averageDistance), all_paths[path][i],str(num_good)))\n",
    "\n",
    "            transformer = transform.EuclideanTransform()\n",
    "            try:\n",
    "                warped_img2, warped_pts2 = apply_transform(original_moving, original_target, moving_pts, target_pts, transformer=transformer)\n",
    "            except:\n",
    "                if dev_mode == True:\n",
    "                    f.write('fail on' + str(i) + '-1 apply_transform')\n",
    "                    f.close()\n",
    "                break\n",
    "            warped_img2 = img_as_ubyte(warped_img2)\n",
    "            if num_good<_num_good or averageDistance > _averageDistance:      ## UPDATED\n",
    "                break\n",
    "            else:\n",
    "                d_reg[int(stainord), 'i-1'] = warped_img2\n",
    "                if dev_mode == True:\n",
    "                    io.imsave(str(i)+'_i-1_' + (os.path.basename(os.path.splitext(all_paths[path][i])[0]+ '_reg.tiff')), warped_img2)\n",
    "\n",
    "                continue\n",
    "            break\n",
    "    if dev_mode == True:\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    \n",
    "    #combine mask on d_reg\n",
    "    l_reg_sort = [list(g) for k, g in groupby(list(d_reg.keys()), key = lambda x: x[0])]  # sort files by first character (original and i-1 image will both have same number preceding) # need to update to allow for numbers > 10\n",
    "    l_reg_filter = []\n",
    "    for h in l_reg_sort:         # if i-1 image exists, take it if not take the regular registered image\n",
    "        if len(h) == 1:\n",
    "            l_reg_filter.append(d_reg[h[0]])\n",
    "        if len(h) == 2:\n",
    "            l_reg_filter.append(d_reg[h[1]])\n",
    "\n",
    "    combined_mask = np.full((l_reg_filter[0].shape[0],l_reg_filter[0].shape[1]), True)    # create empty mask file with same shape as images\n",
    "    for k in l_reg_filter:\n",
    "        if len(l_reg_filter) == len(stain_order):     # only take tiles where we have an image for every stain in stain_order\n",
    "            k[np.all(k == (0,0,0), axis = -1)] = (255, 255,255)  # set black registration gaps to white\n",
    "            gaus = gaussian(k, 6, multichannel = True)  # apply gaussian blur to image, returns float\n",
    "\n",
    "            thresh = cell_reg_thresh    # Try range of values to determine ideal threshold                            ##### UPDATED, ADDED TO GLOBAL VARIABLES\n",
    "            mask_gaus = (gaus[:,:,1] ==0) | ((gaus[:,:,0] > thresh) | (gaus[:,:,1] > thresh) | (gaus[:,:,2] > thresh)) # masks any image area where pixel value is either 0 or above threshold on any of the 3 channels\n",
    "            combined_mask = np.logical_and(combined_mask , mask_gaus)\n",
    "        else:\n",
    "            break\n",
    "    else:     \n",
    "        clean_mask = remove_small_objects(combined_mask, min_size=2000)   # can edit size of small objects / small holes here\n",
    "        clean_mask = remove_small_holes(clean_mask, area_threshold=2000)\n",
    "        if dev_mode == True:\n",
    "            io.imsave('{}_{}_{}_CombinedMask.tiff'.format(slide, row, col), img_as_ubyte(clean_mask))   #save combined mask file\n",
    "        arrays = []\n",
    "        for y in l_reg_filter:\n",
    "            y[np.all(y == (0,0,0), axis = -1)] = (250, 250,250)\n",
    "            y[clean_mask] = 250\n",
    "            arrays.append(y)\n",
    "        stack = np.stack(arrays, axis = 0)\n",
    "        io.imsave('{}_{}_{}_stack.tiff'.format(slide, row, col), stack)\n",
    "        if dev_mode == True:\n",
    "            demo = l_reg_filter[0]   # apply mask to index 0 stain as example       \n",
    "            gaps = demo[:,:,1] == 0\n",
    "            demo[gaps] = 250\n",
    "            demo[clean_mask] = 250\n",
    "            io.imsave('{}_{}_{}_demo.tiff'.format(slide, row, col), demo)   # save an example of the mask on image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Base Alignment of Full Slide Images #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.88 ms (started: 2022-08-11 10:13:40 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# create subfolders in output folder\n",
    "os.chdir(output_dir)\n",
    "os.makedirs(output_dir + '/BaseImages', exist_ok = True)\n",
    "os.makedirs(input_folder, exist_ok = True)\n",
    "os.makedirs(stitch_folder, exist_ok = True)\n",
    "os.makedirs(crops_folder, exist_ok = True)\n",
    "os.makedirs(labels_folder, exist_ok = True)\n",
    "os.makedirs(plots_folder, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected PD-1 N16-244-2A file = /home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/PD-1/N16-244-2A.tiff\n",
      "Expected PD-L1 N16-244-2A file = /home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/PD-L1/N16-244-2A.tiff\n",
      "Expected CD68 N16-244-2A file = /home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/CD68/N16-244-2A.tiff\n",
      "Expected CD3 N16-244-2A file = /home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/CD3/N16-244-2A.tiff\n",
      "Expected FoxP3 N16-244-2A file = /home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/FoxP3/N16-244-2A.tiff\n",
      "Expected Iba-1 N16-244-2A file = /home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/Iba-1/N16-244-2A.tiff\n",
      "Expected CD8 N16-244-2A file = /home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/CD8/N16-244-2A.tiff\n",
      "Expected CD4 N16-244-2A file = /home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/CD4/N16-244-2A.tiff\n",
      "time: 3 ms (started: 2022-08-11 10:13:40 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# create list of lists of paths to raw images for each slide stains in imaged order\n",
    "all_slides = []\n",
    "for y in slide_id:\n",
    "    slide_names = []\n",
    "    for root, dirs, files in os.walk(raw_img_dir):\n",
    "        for i in files:\n",
    "            if os.path.splitext(i)[1] in ['.scn', '.svs', '.tiff']:\n",
    "                if os.path.splitext(i)[0] == y:\n",
    "                    stain = os.path.basename(os.path.dirname(os.path.join(root, i)))\n",
    "                    stain_ord = stain_order.index(stain)\n",
    "                    slide_names.append(tuple([stain_ord, os.path.join(root, i)]))\n",
    "\n",
    "    slide_names = sorted(slide_names)\n",
    "    slide_names = [x[1] for x in slide_names]\n",
    "    all_slides.append(slide_names)\n",
    "\n",
    "#checks that for each sample, raw image file exists for every file and are in the correct order\n",
    "if dev_mode == True:\n",
    "    for index, l in enumerate(all_slides):\n",
    "        for indexx, ll in enumerate(l):\n",
    "            print('Expected ' + stain_order[indexx] +' '+ slide_id[index] + ' file = ' +  ll)\n",
    "            assert os.path.basename(os.path.dirname(ll)) == stain_order[indexx] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slide index = 0 slide_id = N16-244-2A\n",
      "time: 792 µs (started: 2022-08-11 10:13:42 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Choose slide to run pipeline on\n",
    "if batch_mode == False:\n",
    "    for index, slide in enumerate(slide_id):\n",
    "        print('slide index = '+str(index) + ' slide_id = ' +slide)\n",
    "\n",
    "    test_slide_index = 0   # if batch_mode = False, set index of single slide to run workflow on here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/PD-1/N16-244-2A.tiff (4096, 4096, 3)\n",
      "/home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/PD-L1/N16-244-2A.tiff (4096, 4096, 3)\n",
      "/home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/CD68/N16-244-2A.tiff (4096, 4096, 3)\n",
      "/home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/CD3/N16-244-2A.tiff (4096, 4096, 3)\n",
      "/home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/FoxP3/N16-244-2A.tiff (4096, 4096, 3)\n",
      "/home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/Iba-1/N16-244-2A.tiff (4096, 4096, 3)\n",
      "/home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/CD8/N16-244-2A.tiff (4096, 4096, 3)\n",
      "/home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages/CD4/N16-244-2A.tiff (4096, 4096, 3)\n",
      "time: 3.84 s (started: 2022-08-11 10:13:42 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Open and resave raw images cropped to the nearest multiple of tile_x and tile_y pixels in x, y dimensions\n",
    "if batch_mode == False:\n",
    "    for y in [all_slides[test_slide_index]]:\n",
    "        l_shapes = []\n",
    "        for index, slide in enumerate(y):\n",
    "            img = tifffile.imread(slide, key = goalRes(slide))    # watch out for if within an image set, output shapes change\n",
    "            crop_row = (img.shape[0]%tile_x)\n",
    "            crop_col = (img.shape[1]%tile_y)\n",
    "            out_image = crop(img, ((int(math.floor(crop_row/2)), int(math.ceil(crop_row/2))), (int(math.floor(crop_col/2)), int(math.ceil(crop_col/2))), (0,0)), copy = False)\n",
    "            if dev_mode == True:\n",
    "                print(slide , out_image.shape)\n",
    "            l_shapes.append(out_image.shape)\n",
    "            io.imsave(output_dir + f'/BaseImages/{index}_Base_{os.path.basename(os.path.splitext(slide)[0])}_{os.path.basename(os.path.dirname(slide))}.tiff', out_image)               #### UPDATED\n",
    "        assert len(set(l_shapes))<=1 # ensures that all images are the same dimensions\n",
    "else: \n",
    "    for y in all_slides:\n",
    "        l_shapes = []\n",
    "        for index, slide in enumerate(y):\n",
    "            img = tifffile.imread(slide, key = goalRes(slide))    # watch out for if within an image set, output shapes change\n",
    "            crop_row = (img.shape[0]%tile_x)\n",
    "            crop_col = (img.shape[1]%tile_y)\n",
    "            out_image = crop(img, ((int(math.floor(crop_row/2)), int(math.ceil(crop_row/2))), (int(math.floor(crop_col/2)), int(math.ceil(crop_col/2))), (0,0)), copy = False)\n",
    "            if dev_mode == True:\n",
    "                print(slide , out_image.shape)\n",
    "            l_shapes.append(out_image.shape)\n",
    "            io.imsave(output_dir + f'/BaseImages/{index}_Base_{os.path.basename(os.path.splitext(slide)[0])}_{os.path.basename(os.path.dirname(slide))}.tiff', out_image)        #### UPDATED\n",
    "        assert len(set(l_shapes))<=1 # ensures that all images are the same dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 995 µs (started: 2022-08-11 10:13:46 -05:00)\n"
     ]
    }
   ],
   "source": [
    "l_base = []\n",
    "for y in slide_id:\n",
    "    slide_names = []\n",
    "    for files in os.listdir(output_dir + '/BaseImages'):\n",
    "        _, _, pat, stain = os.path.splitext(files)[0].split('_')\n",
    "        if pat == y:\n",
    "            stain_ord = stain_order.index(stain)\n",
    "            slide_names.append(tuple([stain_ord, os.path.join(output_dir+ '/BaseImages/' ,files)]))\n",
    "    slide_names = sorted(slide_names)\n",
    "    slide_names = [x[1] for x in slide_names]\n",
    "    l_base.append(slide_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 19s (started: 2022-08-11 10:13:46 -05:00)\n"
     ]
    }
   ],
   "source": [
    "if batch_mode == False:\n",
    "    base_align(l_base[test_slide_index])\n",
    "else:\n",
    "    Parallel(n_jobs = 1)(delayed(base_align)(file) for file in tqdm(l_base))   # increase n_jobs if file size is small or system memory is being under utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.1 ms (started: 2022-08-11 10:15:06 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# collects all base_aligned files \n",
    "l_files = sorted(glob.glob(input_folder + '/*.tiff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dev mode:open base aligned images in napari \n",
    "if dev_mode == True and batch_mode == False:\n",
    "    with napari.gui_qt():\n",
    "        viewer = napari.view_image(io.imread(l_files[0]), name = os.path.basename(l_files[0]))\n",
    "        for file in l_files[1:]:\n",
    "            viewer.add_image(io.imread(file), name = os.path.basename(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tile Image and Register #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 605.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.3 s (started: 2022-08-11 10:15:06 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Crop and save all tiles from all base aligned images\n",
    "Parallel(n_jobs = int(available_cores * 0.9))(delayed(tileSave)(file)for file in tqdm(l_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list tilemaps and open in napari\n",
    "if dev_mode == True and batch_mode == False:\n",
    "    l_tilemaps = sorted(glob.glob(output_folder + '/*_tilemap.tiff'))  \n",
    "    with napari.gui_qt():\n",
    "        viewer = napari.view_image(io.imread(l_tilemaps[0]), name = os.path.basename(l_tilemaps[0]))\n",
    "        for file in l_tilemaps[1:]:\n",
    "            viewer.add_image(io.imread(file), name = os.path.basename(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.6 ms (started: 2022-08-11 10:15:13 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# list all tiles for a slide\n",
    "l_allfiles = []\n",
    "for root, dirs, files in os.walk(output_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('_reg.tiff') | file.endswith('_tilemap.tiff'):\n",
    "            continue\n",
    "        if os.path.basename(root) == 'mask':\n",
    "            continue\n",
    "        if file.endswith('.tiff'):\n",
    "            l_allfiles.append(os.path.join(root,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.57 ms (started: 2022-08-11 10:15:13 -05:00)\n"
     ]
    }
   ],
   "source": [
    "#Extract metadata for each cropped image from file names\n",
    "metadata = []\n",
    "for path in l_allfiles:\n",
    "    file = os.path.basename(os.path.splitext(path)[0])\n",
    "    slide,stain,row,col = file.split('_')\n",
    "    coord = row+col\n",
    "    stain_ord = stain_order.index(stain)\n",
    "    file_meta = [slide, stain, stain_ord, coord, path]\n",
    "    metadata.append(file_meta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.94 ms (started: 2022-08-11 10:15:13 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Filter all cropped images for first stain as baseline to register all other images to\n",
    "stain0_list = [item for item in metadata if item[1] == stain_order[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.78 ms (started: 2022-08-11 10:15:13 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# For each cropped stain_order[0] image, find all files that are from the matching row - col location of the slide.  \n",
    "# If a matching crop does not exist in all files, eliminate.\n",
    "# Creates dict with key = stain_order[0] path and values = all matching tile paths in stain order\n",
    "# Creates list of all stain_order[0] paths where all stain channel images exist to iterate through\n",
    "all_paths = {}\n",
    "l_stain0_filter = []\n",
    "for coord in stain0_list:\n",
    "    if batch_mode == False:                                                                        #### UPDATED BLOCK TO BELOW\n",
    "        if coord[0] == slide_id[test_slide_index]:\n",
    "            files = [item for item in metadata if (item[0] == coord[0] and item[3] == coord[3])]\n",
    "            if len(files) == len(stain_order):\n",
    "                for i in files:\n",
    "                    files = sorted(files, key = lambda x: x[2])\n",
    "                    if i[1] == stain_order[0]:\n",
    "                        l_stain0_filter.append(i[4])\n",
    "                    file_paths = [item[4] for item in files]\n",
    "                    key = [item[4] for item in files if item[1] == stain_order[0]]\n",
    "                    all_paths[key[0]] = file_paths\n",
    "    else:\n",
    "        files = [item for item in metadata if (item[0] == coord[0] and item[3] == coord[3])]\n",
    "        if len(files) == len(stain_order):\n",
    "            for i in files:\n",
    "                files = sorted(files, key = lambda x: x[2])\n",
    "                if i[1] == stain_order[0]:\n",
    "                    l_stain0_filter.append(i[4])\n",
    "                file_paths = [item[4] for item in files]\n",
    "                key = [item[4] for item in files if item[1] == stain_order[0]]\n",
    "                all_paths[key[0]] = file_paths                                                      #### UPDATED\n",
    "l_stain0_filter = sorted(l_stain0_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 4120.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 18s (started: 2022-08-11 10:15:13 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# registers all image sets per tile\n",
    "Parallel(n_jobs = int(available_cores * 0.9))(delayed(regAll)(file)for file in tqdm(l_stain0_filter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Filter for only \"Good\" Tiles, Crop Stacks + Stitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.13 ms (started: 2022-08-11 10:17:31 -05:00)\n"
     ]
    }
   ],
   "source": [
    "good_tiles = pd.read_csv(output_dir + '/good_tiles.csv', header = None)   # save list of good tiles as good_tiles.csv file in the base output directory.  Column 1 = Row, Column 2 = Column, Column 3 = Slide\n",
    "tiles_in = []\n",
    "for x in good_tiles.values.tolist():\n",
    "    tiles_in.append((int(x[0]), int(x[1]), x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.57 ms (started: 2022-08-11 10:17:31 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# create list of all 'stack' files\n",
    "l_stacks = []\n",
    "for root, dirs, files in os.walk(reg_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('_stack.tiff'):\n",
    "            l_stacks.append(os.path.join(root, file))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.92 ms (started: 2022-08-11 10:17:31 -05:00)\n"
     ]
    }
   ],
   "source": [
    "sort_l_files = [file for file in l_files if os.path.basename(file).startswith('Aligned_0')]   #list of 1 image from each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.49 s (started: 2022-08-11 10:17:31 -05:00)\n"
     ]
    }
   ],
   "source": [
    "for p in sort_l_files:\n",
    "    _, _, _, slide_ID, _ = os.path.basename(p).split('_')\n",
    "\n",
    "    stack_meta = {}\n",
    "    for path in l_stacks:\n",
    "        file = os.path.basename(os.path.splitext(path)[0])\n",
    "        slide,row, col, _ = file.split('_')\n",
    "        int_row = row[1:]\n",
    "        int_col = col[1:]\n",
    "        coord = (int(int_row), int(int_col), str(slide))\n",
    "        if slide == slide_ID:\n",
    "            if coord in tiles_in:                 # Checks if each coord is listed in users's good_tiles list.  Can comment out this line if all tiles are considered 'good'\n",
    "                stack_meta[coord] = path\n",
    "\n",
    "    img = io.imread(p)   # reads image shape without reading full image\n",
    "    num_rows = list(range(int(img.shape[0]/tile_x)))\n",
    "    num_cols = list(range(int(img.shape[1]/tile_y)))\n",
    "    combined = [(f,s, slide) for f in num_rows for s in num_cols] \n",
    "\n",
    "    l_CropPath = []\n",
    "    for item in combined:\n",
    "        if item in stack_meta.keys():\n",
    "            l_CropPath.append((item, stack_meta[item]))\n",
    "        else:\n",
    "            l_CropPath.append((item, 'placeholder'))\n",
    "\n",
    "    l_crops = []\n",
    "    for item in l_CropPath:\n",
    "        if item[1] == 'placeholder':\n",
    "            l_crops.append(np.zeros((len(stain_order),tile_x,tile_y,3), dtype = np.uint8))\n",
    "        else:\n",
    "            row = item[0][0]\n",
    "            col = item[0][1]\n",
    "            if row == 0:\n",
    "                row_top_offset = 0\n",
    "            else:\n",
    "                row_top_offset = overlap_x\n",
    "            if row == int(img.shape[0]/tile_x)-1:\n",
    "                row_bot_offset = 0\n",
    "            else:\n",
    "                row_bot_offset = overlap_x\n",
    "            if col == 0:\n",
    "                col_l_offset = 0\n",
    "            else:\n",
    "                col_l_offset = overlap_y\n",
    "            if col == int(img.shape[1]/tile_y)-1:\n",
    "                col_r_offset = 0\n",
    "            else:\n",
    "                col_r_offset = overlap_y\n",
    "            cropped_img = crop(io.imread(item[1]), ((0,0), (row_top_offset, row_bot_offset), (col_l_offset, col_r_offset), (0,0)), copy = False)\n",
    "            assert cropped_img.shape == (len(stain_order),tile_x, tile_y,3)                                              ##### UPDATED TO BELOW\n",
    "\n",
    "            thresh = cell_reg_thresh\n",
    "            gaus0 = gaussian(cropped_img[0], 6, multichannel = True)\n",
    "            mask_gaus0 = ((gaus0[:,:,0] > thresh) | (gaus0[:,:,1] > thresh) | (gaus0[:,:,2] > thresh)) \n",
    "            gaus = gaussian(cropped_img[-1], 6, multichannel = True)\n",
    "            mask_gaus = ((gaus[:,:,0] > thresh) | (gaus[:,:,1] > thresh) | (gaus[:,:,2] > thresh)) \n",
    "            tissue_loss = (1 - (np.size(mask_gaus)-np.count_nonzero(mask_gaus))/(np.size(mask_gaus0)-np.count_nonzero(mask_gaus0))) * 100\n",
    "\n",
    "            \n",
    "            if tissue_loss <= tissue_loss_thresh:\n",
    "                l_crops.append(cropped_img)\n",
    "                tifffile.imwrite(crops_folder + '/{}_r{}_c{}_cropped.tiff'.format(item[0][2],item[0][0], item[0][1]), cropped_img)    # crop off overlap pixels and save as final registered stack tiles\n",
    "    if stitch_all == True:                                                                                                                     \n",
    "        conc_rows = []                                                                                                                  \n",
    "        for x in range(len(num_rows)):\n",
    "            conc_rows.append((np.concatenate(l_crops[((max(num_cols)+1)*(x)):((max(num_cols)+1)*(x+1))], axis = 2)))\n",
    "        conc_all = np.concatenate(conc_rows, axis = 1)\n",
    "\n",
    "        tifffile.imwrite(stitch_folder + f'/{slide_ID}_stitched.tiff', conc_all)   # stitch all stacks together for visualization      ##### UPDATED TO ABOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Napari Visualization of Stitched Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 689 µs (started: 2022-08-11 10:17:40 -05:00)\n"
     ]
    }
   ],
   "source": [
    "l_full_stitch = []\n",
    "\n",
    "for root, dirs, files in os.walk(output_dir + '/StitchedSlides/'):\n",
    "    for file in files:\n",
    "        if file.endswith('_stitched.tiff'):\n",
    "            l_full_stitch.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import napari\n",
    "if dev_mode == True:\n",
    "    file = l_full_stitch[0]   # edit int value to choose which slide to visualize\n",
    "\n",
    "    with napari.gui_qt():\n",
    "        viewer = napari.view_image(tifffile.imread(file), name = os.path.basename(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Stardist Segmentation of Tiles and Readout Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 190 ms (started: 2022-08-11 10:17:40 -05:00)\n"
     ]
    }
   ],
   "source": [
    "def deconv(image):\n",
    "    if type(image) == str:\n",
    "        img = io.imread(image)\n",
    "        hax  = separate_stains(img, hax_from_rgb)\n",
    "        hema = hax[:,:,0]\n",
    "        h = rescale_intensity(hema, in_range = (0, 1), out_range = 'uint8')   # edit in-range for better isolation of hematoxylin signal\n",
    "        \n",
    "        aec = hax[:,:,1]\n",
    "        a = rescale_intensity(aec, in_range = (0,1), out_range = 'uint8')\n",
    "    elif type(image) == np.ndarray:\n",
    "        hax = separate_stains(image, hax_from_rgb)\n",
    "        hema = hax[:,:,0]\n",
    "        h = rescale_intensity(hema, in_range = (0, 1), out_range = 'uint8')\n",
    "        \n",
    "        aec = hax[:,:,1]\n",
    "        a = rescale_intensity(aec, in_range = (0,1), out_range = 'uint8')\n",
    "    return h, a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network weights from 'weights_best.h5'.\n",
      "Loading thresholds from 'thresholds.json'.\n",
      "Using default values: prob_thresh=0.443913, nms_thresh=0.5.\n",
      "time: 218 ms (started: 2022-08-11 10:17:40 -05:00)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(stardist_dir) \n",
    "model = StarDist2D(None, name='stardist_7_13', basedir='models')\n",
    "\n",
    "axis_norm = (0,1)\n",
    "n_channel = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 154 ms (started: 2022-08-11 10:17:40 -05:00)\n"
     ]
    }
   ],
   "source": [
    "l_cropped_stacks = []   # list of dicts, single dict per slide, within dict:  tile_x, tile_y, slide_id  =  stack array\n",
    "for x in slide_id:\n",
    "    d_cropped_stacks = {}\n",
    "    for root, dirs, files in os.walk(crops_folder):\n",
    "        for i in files:\n",
    "            if i.endswith('_cropped.tiff'):\n",
    "                if i.startswith(x):\n",
    "                    array = io.imread(os.path.join(root,i))\n",
    "                    pat, row, col, _ = i.split('_')\n",
    "                    tile = (int(row[1:]), int(col[1:]), pat)\n",
    "                    d_cropped_stacks[tile] = array\n",
    "    l_cropped_stacks.append(d_cropped_stacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-113a5ec9becf>:24: UserWarning: /home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/Python_Output/Labels/N16-244-2A_r0_c0_seg.tiff is a low contrast image\n",
      "  io.imsave(labels_folder + '/{}_r{}_c{}_seg.tiff'.format(tile[2], tile[0], tile[1]), seg[0])   # saves deconvoluted image that stardist works on\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 25 s (started: 2022-08-11 10:17:40 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# code for reading labels from external source\n",
    "if batch_mode == False: \n",
    "    if stardist_segmentation == True:\n",
    "        for y in [l_cropped_stacks[test_slide_index]]:    #  to only compute for 1 slide\n",
    "            df_all = pd.DataFrame()\n",
    "            d_labels = {}\n",
    "            d_expanded_labels = {}\n",
    "\n",
    "            for tile, array in y.items():\n",
    "                df_int = pd.DataFrame()\n",
    "                seg = deconv(array[seg_channel])                  ## UPDATED\n",
    "                norm = normalize(seg[0], 1, 99.8, axis = axis_norm)\n",
    "                labels, details = model.predict_instances(norm)\n",
    "\n",
    "                d_labels[tile] = labels\n",
    "\n",
    "                exp_lab = expand_labels(labels, distance = exp_lab_dist)   \n",
    "\n",
    "                d_expanded_labels[tile] = exp_lab\n",
    "\n",
    "                #save labels\n",
    "                io.imsave(labels_folder + '/{}_r{}_c{}_labels.tiff'.format(tile[2], tile[0], tile[1]), labels, check_contrast = False)                        ##UPDATED\n",
    "                if dev_mode == True:\n",
    "                    io.imsave(labels_folder + '/{}_r{}_c{}_seg.tiff'.format(tile[2], tile[0], tile[1]), seg[0])   # saves deconvoluted image that stardist works on\n",
    "                    io.imsave(labels_folder + '/{}_r{}_c{}_seg_raw.tiff'.format(tile[2], tile[0], tile[1]), array[seg_channel])  # saves raw pre-deconvolution image    ## UPDATED\n",
    "\n",
    "\n",
    "                for i in range(array.shape[0]):\n",
    "                    int_img = deconv(array[i])[1]\n",
    "                    nuc_props = regionprops_table(label_image = labels, intensity_image = int_img, \n",
    "                                              properties = ('label', 'centroid', 'mean_intensity'))\n",
    "                    nuc_data = pd.DataFrame(nuc_props)\n",
    "                    nuc_data.rename(columns = {'mean_intensity': 'mean_intensity_' + stain_order[i] + '_nuc'}, inplace = True)\n",
    "\n",
    "                    cell_props = regionprops_table(label_image = exp_lab, intensity_image = int_img, \n",
    "                                              properties = ('label', 'mean_intensity'))\n",
    "                    cell_data = pd.DataFrame(cell_props)\n",
    "                    cell_data.rename(columns = {'mean_intensity': 'mean_intensity_' + stain_order[i] + '_cell'}, inplace = True)\n",
    "\n",
    "                    data = nuc_data.merge(cell_data, on = 'label')\n",
    "\n",
    "                    if i != 0:\n",
    "                        data = data.drop(['centroid-1', 'centroid-0'], axis = 1)\n",
    "\n",
    "                    if i == 0:\n",
    "                        data.insert(1, 'Tile', str(tile))\n",
    "                        data.insert(1, 'Tile_row', tile[0])\n",
    "                        data.insert(2, 'Tile_col', tile[1])\n",
    "                        data['Global_y'] = data['centroid-0']+(data['Tile_row']*float(tile_x))\n",
    "                        data['Global_x'] = data['centroid-1']+(data['Tile_col']*float(tile_y))\n",
    "                        df_int = data\n",
    "                    else:\n",
    "                        df_int = df_int.merge(data, on = 'label')\n",
    "                try:\n",
    "                    df_all = df_all.append(df_int, ignore_index = True)\n",
    "                except:\n",
    "                    df_all = df_int\n",
    "            df_all.to_csv(output_dir + '/{}_export.csv'.format(tile[2]))\n",
    "    else:\n",
    "        d_labels = {}\n",
    "        for ii in glob.glob(labels_folder + '/*labels.tiff'):\n",
    "            pat, row, col, _ = os.path.basename(ii).split('_')\n",
    "            tile = (int(row[1:]), int(col[1:]), pat)\n",
    "        for y in [l_cropped_stacks[test_slide_index]]:\n",
    "            df_all = pd.DataFrame()\n",
    "\n",
    "            for tile, array in y.items():\n",
    "                labels = d_labels[tile]\n",
    "                exp_lab = expand_labels(labels, distance = exp_lab_dist)   \n",
    "\n",
    "                for i in range(array.shape[0]):\n",
    "                    int_img = deconv(array[i])[1]\n",
    "                    nuc_props = regionprops_table(label_image = labels, intensity_image = int_img, \n",
    "                                              properties = ('label', 'centroid', 'mean_intensity'))\n",
    "                    nuc_data = pd.DataFrame(nuc_props)\n",
    "                    nuc_data.rename(columns = {'mean_intensity': 'mean_intensity_' + stain_order[i] + '_nuc'}, inplace = True)\n",
    "\n",
    "                    cell_props = regionprops_table(label_image = exp_lab, intensity_image = int_img, \n",
    "                                              properties = ('label', 'mean_intensity'))\n",
    "                    cell_data = pd.DataFrame(cell_props)\n",
    "                    cell_data.rename(columns = {'mean_intensity': 'mean_intensity_' + stain_order[i] + '_cell'}, inplace = True)\n",
    "\n",
    "                    data = nuc_data.merge(cell_data, on = 'label')\n",
    "\n",
    "                    if i != 0:\n",
    "                        data = data.drop(['centroid-1', 'centroid-0'], axis = 1)\n",
    "\n",
    "                    if i == 0:\n",
    "                        data.insert(1, 'Tile', str(tile))\n",
    "                        data.insert(1, 'Tile_row', tile[0])\n",
    "                        data.insert(2, 'Tile_col', tile[1])\n",
    "                        data['Global_y'] = data['centroid-0']+(data['Tile_row']*float(tile_x))\n",
    "                        data['Global_x'] = data['centroid-1']+(data['Tile_col']*float(tile_y))\n",
    "                        df_int = data\n",
    "                    else:\n",
    "                        df_int = df_int.merge(data, on = 'label')\n",
    "                try:\n",
    "                    df_all = df_all.append(df_int, ignore_index = True)\n",
    "                except:\n",
    "                    df_all = df_int\n",
    "            df_all.to_csv(output_dir + '/{}_export.csv'.format(tile[2]))\n",
    "else:\n",
    "    if stardist_segmentation == True:\n",
    "        for y in l_cropped_stacks:  \n",
    "            df_all = pd.DataFrame()\n",
    "            d_labels = {}\n",
    "            d_expanded_labels = {}\n",
    "\n",
    "            for tile, array in y.items():\n",
    "                df_int = pd.DataFrame()\n",
    "                seg = deconv(array[seg_channel])               ## UPDATED\n",
    "                norm = normalize(seg[0], 1, 99.8, axis = axis_norm)\n",
    "                labels, details = model.predict_instances(norm)\n",
    "\n",
    "                d_labels[tile] = labels\n",
    "\n",
    "                exp_lab = expand_labels(labels, distance = exp_lab_dist)   \n",
    "\n",
    "                d_expanded_labels[tile] = exp_lab\n",
    "\n",
    "                #save labels\n",
    "                io.imsave(labels_folder + '/{}_r{}_c{}_labels.tiff'.format(tile[2], tile[0], tile[1]), labels, check_contrast = False)                          ## UPDATED\n",
    "                if dev_mode == True:\n",
    "                    io.imsave(labels_folder + '/{}_r{}_c{}_seg.tiff'.format(tile[2], tile[0], tile[1]), seg[0])   # saves deconvoluted image that stardist works on\n",
    "                    io.imsave(labels_folder + '/{}_r{}_c{}_seg_raw.tiff'.format(tile[2], tile[0], tile[1]), array[seg_channel])  # saves raw pre-deconvolution image    ### UPDATED\n",
    "\n",
    "\n",
    "                for i in range(array.shape[0]):\n",
    "                    int_img = deconv(array[i])[1]\n",
    "                    nuc_props = regionprops_table(label_image = labels, intensity_image = int_img, \n",
    "                                              properties = ('label', 'centroid', 'mean_intensity'))\n",
    "                    nuc_data = pd.DataFrame(nuc_props)\n",
    "                    nuc_data.rename(columns = {'mean_intensity': 'mean_intensity_' + stain_order[i] + '_nuc'}, inplace = True)\n",
    "\n",
    "                    cell_props = regionprops_table(label_image = exp_lab, intensity_image = int_img, \n",
    "                                              properties = ('label', 'mean_intensity'))\n",
    "                    cell_data = pd.DataFrame(cell_props)\n",
    "                    cell_data.rename(columns = {'mean_intensity': 'mean_intensity_' + stain_order[i] + '_cell'}, inplace = True)\n",
    "\n",
    "                    data = nuc_data.merge(cell_data, on = 'label')\n",
    "\n",
    "                    if i != 0:\n",
    "                        data = data.drop(['centroid-1', 'centroid-0'], axis = 1)\n",
    "\n",
    "                    if i == 0:\n",
    "                        data.insert(1, 'Tile', str(tile))\n",
    "                        data.insert(1, 'Tile_row', tile[0])\n",
    "                        data.insert(2, 'Tile_col', tile[1])\n",
    "                        data['Global_y'] = data['centroid-0']+(data['Tile_row']*float(tile_x))\n",
    "                        data['Global_x'] = data['centroid-1']+(data['Tile_col']*float(tile_y))\n",
    "                        df_int = data\n",
    "                    else:\n",
    "                        df_int = df_int.merge(data, on = 'label')\n",
    "                try:\n",
    "                    df_all = df_all.append(df_int, ignore_index = True)\n",
    "                except:\n",
    "                    df_all = df_int\n",
    "            df_all.to_csv(output_dir + '/{}_export.csv'.format(tile[2]))\n",
    "    else:                                                              ### UPDATED TO END\n",
    "        d_labels = {}\n",
    "        for ii in glob.glob(labels_folder + '/*labels.tiff'):\n",
    "            pat, row, col, _ = os.path.basename(ii).split('_')\n",
    "            tile = (int(row[1:]), int(col[1:]), pat)\n",
    "        for y in l_cropped_stacks:\n",
    "            df_all = pd.DataFrame()\n",
    "\n",
    "            for tile, array in y.items():\n",
    "                labels = d_labels[tile]\n",
    "                exp_lab = expand_labels(labels, distance = exp_lab_dist)   \n",
    "\n",
    "                for i in range(array.shape[0]):\n",
    "                    int_img = deconv(array[i])[1]\n",
    "                    nuc_props = regionprops_table(label_image = labels, intensity_image = int_img, \n",
    "                                              properties = ('label', 'centroid', 'mean_intensity'))\n",
    "                    nuc_data = pd.DataFrame(nuc_props)\n",
    "                    nuc_data.rename(columns = {'mean_intensity': 'mean_intensity_' + stain_order[i] + '_nuc'}, inplace = True)\n",
    "\n",
    "                    cell_props = regionprops_table(label_image = exp_lab, intensity_image = int_img, \n",
    "                                              properties = ('label', 'mean_intensity'))\n",
    "                    cell_data = pd.DataFrame(cell_props)\n",
    "                    cell_data.rename(columns = {'mean_intensity': 'mean_intensity_' + stain_order[i] + '_cell'}, inplace = True)\n",
    "\n",
    "                    data = nuc_data.merge(cell_data, on = 'label')\n",
    "\n",
    "                    if i != 0:\n",
    "                        data = data.drop(['centroid-1', 'centroid-0'], axis = 1)\n",
    "\n",
    "                    if i == 0:\n",
    "                        data.insert(1, 'Tile', str(tile))\n",
    "                        data.insert(1, 'Tile_row', tile[0])\n",
    "                        data.insert(2, 'Tile_col', tile[1])\n",
    "                        data['Global_y'] = data['centroid-0']+(data['Tile_row']*float(tile_x))\n",
    "                        data['Global_x'] = data['centroid-1']+(data['Tile_col']*float(tile_y))\n",
    "                        df_int = data\n",
    "                    else:\n",
    "                        df_int = df_int.merge(data, on = 'label')\n",
    "                try:\n",
    "                    df_all = df_all.append(df_int, ignore_index = True)\n",
    "                except:\n",
    "                    df_all = df_int\n",
    "            df_all.to_csv(output_dir + '/{}_export.csv'.format(tile[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 162 ms (started: 2022-08-11 10:18:05 -05:00)\n"
     ]
    }
   ],
   "source": [
    "df_all.to_csv(output_dir + '/df_all.csv')    # save all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.7 ms (started: 2022-08-11 10:18:06 -05:00)\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_csv(output_dir + '/df_all.csv')   # load all features.  Can skip to this step if above has already been run and only visualizations below are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>Tile_row</th>\n",
       "      <th>Tile_col</th>\n",
       "      <th>Tile</th>\n",
       "      <th>centroid-0</th>\n",
       "      <th>centroid-1</th>\n",
       "      <th>mean_intensity_PD-1_nuc</th>\n",
       "      <th>mean_intensity_PD-1_cell</th>\n",
       "      <th>Global_y</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_intensity_CD3_nuc</th>\n",
       "      <th>mean_intensity_CD3_cell</th>\n",
       "      <th>mean_intensity_FoxP3_nuc</th>\n",
       "      <th>mean_intensity_FoxP3_cell</th>\n",
       "      <th>mean_intensity_Iba-1_nuc</th>\n",
       "      <th>mean_intensity_Iba-1_cell</th>\n",
       "      <th>mean_intensity_CD8_nuc</th>\n",
       "      <th>mean_intensity_CD8_cell</th>\n",
       "      <th>mean_intensity_CD4_nuc</th>\n",
       "      <th>mean_intensity_CD4_cell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(0, 0, 'N16-244-2A')</td>\n",
       "      <td>853.256944</td>\n",
       "      <td>1940.020833</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>853.256944</td>\n",
       "      <td>...</td>\n",
       "      <td>4.118056</td>\n",
       "      <td>3.007937</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>1.884354</td>\n",
       "      <td>6.840278</td>\n",
       "      <td>8.104308</td>\n",
       "      <td>4.291667</td>\n",
       "      <td>3.853741</td>\n",
       "      <td>2.402778</td>\n",
       "      <td>2.280045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(0, 0, 'N16-244-2A')</td>\n",
       "      <td>1072.938144</td>\n",
       "      <td>1791.041237</td>\n",
       "      <td>19.917526</td>\n",
       "      <td>17.334286</td>\n",
       "      <td>1072.938144</td>\n",
       "      <td>...</td>\n",
       "      <td>3.247423</td>\n",
       "      <td>3.625714</td>\n",
       "      <td>2.927835</td>\n",
       "      <td>2.854286</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>11.585714</td>\n",
       "      <td>5.865979</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>3.628866</td>\n",
       "      <td>2.637143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(0, 0, 'N16-244-2A')</td>\n",
       "      <td>1950.520661</td>\n",
       "      <td>1896.396694</td>\n",
       "      <td>66.669421</td>\n",
       "      <td>30.937500</td>\n",
       "      <td>1950.520661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330579</td>\n",
       "      <td>1.614583</td>\n",
       "      <td>1.628099</td>\n",
       "      <td>2.002604</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>1.736979</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>3.130208</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(0, 0, 'N16-244-2A')</td>\n",
       "      <td>1691.153846</td>\n",
       "      <td>1844.589744</td>\n",
       "      <td>11.025641</td>\n",
       "      <td>9.173832</td>\n",
       "      <td>1691.153846</td>\n",
       "      <td>...</td>\n",
       "      <td>2.871795</td>\n",
       "      <td>3.153271</td>\n",
       "      <td>1.782051</td>\n",
       "      <td>2.218692</td>\n",
       "      <td>96.512821</td>\n",
       "      <td>50.160748</td>\n",
       "      <td>13.653846</td>\n",
       "      <td>8.413084</td>\n",
       "      <td>13.243590</td>\n",
       "      <td>10.573832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(0, 0, 'N16-244-2A')</td>\n",
       "      <td>1750.102564</td>\n",
       "      <td>1613.730769</td>\n",
       "      <td>11.064103</td>\n",
       "      <td>9.180527</td>\n",
       "      <td>1750.102564</td>\n",
       "      <td>...</td>\n",
       "      <td>2.435897</td>\n",
       "      <td>2.261663</td>\n",
       "      <td>2.589744</td>\n",
       "      <td>2.235294</td>\n",
       "      <td>3.525641</td>\n",
       "      <td>8.261663</td>\n",
       "      <td>5.115385</td>\n",
       "      <td>4.172414</td>\n",
       "      <td>3.102564</td>\n",
       "      <td>3.095335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6406</th>\n",
       "      <td>6406</td>\n",
       "      <td>1290</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(1, 0, 'N16-244-2A')</td>\n",
       "      <td>354.644295</td>\n",
       "      <td>1623.570470</td>\n",
       "      <td>12.463087</td>\n",
       "      <td>9.936599</td>\n",
       "      <td>2402.644295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208054</td>\n",
       "      <td>2.693084</td>\n",
       "      <td>1.154362</td>\n",
       "      <td>2.152738</td>\n",
       "      <td>6.147651</td>\n",
       "      <td>15.162824</td>\n",
       "      <td>2.859060</td>\n",
       "      <td>4.204611</td>\n",
       "      <td>3.073826</td>\n",
       "      <td>4.367435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6407</th>\n",
       "      <td>6407</td>\n",
       "      <td>1291</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(1, 0, 'N16-244-2A')</td>\n",
       "      <td>228.138889</td>\n",
       "      <td>1193.875000</td>\n",
       "      <td>17.145833</td>\n",
       "      <td>13.632184</td>\n",
       "      <td>2276.138889</td>\n",
       "      <td>...</td>\n",
       "      <td>2.659722</td>\n",
       "      <td>3.329885</td>\n",
       "      <td>1.277778</td>\n",
       "      <td>2.420690</td>\n",
       "      <td>1.631944</td>\n",
       "      <td>2.585057</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>3.273563</td>\n",
       "      <td>2.284722</td>\n",
       "      <td>2.359770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6408</th>\n",
       "      <td>6408</td>\n",
       "      <td>1292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(1, 0, 'N16-244-2A')</td>\n",
       "      <td>1611.720430</td>\n",
       "      <td>1737.817204</td>\n",
       "      <td>6.075269</td>\n",
       "      <td>6.437659</td>\n",
       "      <td>3659.720430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.267176</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.152672</td>\n",
       "      <td>0.989247</td>\n",
       "      <td>1.328244</td>\n",
       "      <td>1.677419</td>\n",
       "      <td>1.328244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6409</th>\n",
       "      <td>6409</td>\n",
       "      <td>1293</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(1, 0, 'N16-244-2A')</td>\n",
       "      <td>1144.871681</td>\n",
       "      <td>1279.389381</td>\n",
       "      <td>13.778761</td>\n",
       "      <td>9.786315</td>\n",
       "      <td>3192.871681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628319</td>\n",
       "      <td>0.594238</td>\n",
       "      <td>0.057522</td>\n",
       "      <td>0.442977</td>\n",
       "      <td>0.216814</td>\n",
       "      <td>0.363745</td>\n",
       "      <td>1.057522</td>\n",
       "      <td>1.319328</td>\n",
       "      <td>0.526549</td>\n",
       "      <td>0.786315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6410</th>\n",
       "      <td>6410</td>\n",
       "      <td>1294</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(1, 0, 'N16-244-2A')</td>\n",
       "      <td>1089.795918</td>\n",
       "      <td>1093.714286</td>\n",
       "      <td>16.107143</td>\n",
       "      <td>14.353994</td>\n",
       "      <td>3137.795918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760204</td>\n",
       "      <td>1.221763</td>\n",
       "      <td>0.173469</td>\n",
       "      <td>0.957300</td>\n",
       "      <td>0.255102</td>\n",
       "      <td>2.979339</td>\n",
       "      <td>2.178571</td>\n",
       "      <td>2.786501</td>\n",
       "      <td>2.020408</td>\n",
       "      <td>2.670799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6411 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  label  Tile_row  Tile_col                  Tile  \\\n",
       "0              0      1         0         0  (0, 0, 'N16-244-2A')   \n",
       "1              1      2         0         0  (0, 0, 'N16-244-2A')   \n",
       "2              2      3         0         0  (0, 0, 'N16-244-2A')   \n",
       "3              3      4         0         0  (0, 0, 'N16-244-2A')   \n",
       "4              4      5         0         0  (0, 0, 'N16-244-2A')   \n",
       "...          ...    ...       ...       ...                   ...   \n",
       "6406        6406   1290         1         0  (1, 0, 'N16-244-2A')   \n",
       "6407        6407   1291         1         0  (1, 0, 'N16-244-2A')   \n",
       "6408        6408   1292         1         0  (1, 0, 'N16-244-2A')   \n",
       "6409        6409   1293         1         0  (1, 0, 'N16-244-2A')   \n",
       "6410        6410   1294         1         0  (1, 0, 'N16-244-2A')   \n",
       "\n",
       "       centroid-0   centroid-1  mean_intensity_PD-1_nuc  \\\n",
       "0      853.256944  1940.020833                 9.250000   \n",
       "1     1072.938144  1791.041237                19.917526   \n",
       "2     1950.520661  1896.396694                66.669421   \n",
       "3     1691.153846  1844.589744                11.025641   \n",
       "4     1750.102564  1613.730769                11.064103   \n",
       "...           ...          ...                      ...   \n",
       "6406   354.644295  1623.570470                12.463087   \n",
       "6407   228.138889  1193.875000                17.145833   \n",
       "6408  1611.720430  1737.817204                 6.075269   \n",
       "6409  1144.871681  1279.389381                13.778761   \n",
       "6410  1089.795918  1093.714286                16.107143   \n",
       "\n",
       "      mean_intensity_PD-1_cell     Global_y  ...  mean_intensity_CD3_nuc  \\\n",
       "0                    10.000000   853.256944  ...                4.118056   \n",
       "1                    17.334286  1072.938144  ...                3.247423   \n",
       "2                    30.937500  1950.520661  ...                0.330579   \n",
       "3                     9.173832  1691.153846  ...                2.871795   \n",
       "4                     9.180527  1750.102564  ...                2.435897   \n",
       "...                        ...          ...  ...                     ...   \n",
       "6406                  9.936599  2402.644295  ...                0.208054   \n",
       "6407                 13.632184  2276.138889  ...                2.659722   \n",
       "6408                  6.437659  3659.720430  ...                0.354839   \n",
       "6409                  9.786315  3192.871681  ...                0.628319   \n",
       "6410                 14.353994  3137.795918  ...                0.760204   \n",
       "\n",
       "      mean_intensity_CD3_cell  mean_intensity_FoxP3_nuc  \\\n",
       "0                    3.007937                  1.625000   \n",
       "1                    3.625714                  2.927835   \n",
       "2                    1.614583                  1.628099   \n",
       "3                    3.153271                  1.782051   \n",
       "4                    2.261663                  2.589744   \n",
       "...                       ...                       ...   \n",
       "6406                 2.693084                  1.154362   \n",
       "6407                 3.329885                  1.277778   \n",
       "6408                 0.267176                  0.666667   \n",
       "6409                 0.594238                  0.057522   \n",
       "6410                 1.221763                  0.173469   \n",
       "\n",
       "      mean_intensity_FoxP3_cell  mean_intensity_Iba-1_nuc  \\\n",
       "0                      1.884354                  6.840278   \n",
       "1                      2.854286                 22.000000   \n",
       "2                      2.002604                  0.471074   \n",
       "3                      2.218692                 96.512821   \n",
       "4                      2.235294                  3.525641   \n",
       "...                         ...                       ...   \n",
       "6406                   2.152738                  6.147651   \n",
       "6407                   2.420690                  1.631944   \n",
       "6408                   0.732824                  0.096774   \n",
       "6409                   0.442977                  0.216814   \n",
       "6410                   0.957300                  0.255102   \n",
       "\n",
       "      mean_intensity_Iba-1_cell  mean_intensity_CD8_nuc  \\\n",
       "0                      8.104308                4.291667   \n",
       "1                     11.585714                5.865979   \n",
       "2                      1.736979                3.363636   \n",
       "3                     50.160748               13.653846   \n",
       "4                      8.261663                5.115385   \n",
       "...                         ...                     ...   \n",
       "6406                  15.162824                2.859060   \n",
       "6407                   2.585057                2.555556   \n",
       "6408                   0.152672                0.989247   \n",
       "6409                   0.363745                1.057522   \n",
       "6410                   2.979339                2.178571   \n",
       "\n",
       "      mean_intensity_CD8_cell  mean_intensity_CD4_nuc  mean_intensity_CD4_cell  \n",
       "0                    3.853741                2.402778                 2.280045  \n",
       "1                    4.720000                3.628866                 2.637143  \n",
       "2                    3.130208                2.000000                 1.958333  \n",
       "3                    8.413084               13.243590                10.573832  \n",
       "4                    4.172414                3.102564                 3.095335  \n",
       "...                       ...                     ...                      ...  \n",
       "6406                 4.204611                3.073826                 4.367435  \n",
       "6407                 3.273563                2.284722                 2.359770  \n",
       "6408                 1.328244                1.677419                 1.328244  \n",
       "6409                 1.319328                0.526549                 0.786315  \n",
       "6410                 2.786501                2.020408                 2.670799  \n",
       "\n",
       "[6411 rows x 25 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.9 ms (started: 2022-08-11 10:18:06 -05:00)\n"
     ]
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole Slide Psudocolor Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_all.columns:\n",
    "    print(column)\n",
    "feature = 'mean_intensity_PD-1_cell'    # string for specific feature of interest. Copy and paste from printed strings below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# figure of all detections scatter of full slide\n",
    "if batch_mode == False:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = sns.scatterplot(data = df_all, x = 'Global_x', y = 'Global_y', hue = feature, hue_norm = (0, 100), palette = 'viridis', linewidth=0, s =10)\n",
    "    ax.set_aspect(aspect = 1)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.savefig(f'{plots_folder}/{feature}_{slide_id[test_slide_index]}.pdf', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Voronoi Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create voronoi diagram for selected tile with each cell polygon's color indicating the measured intensity of a feature of interest\n",
    "# Next cell will list out available options for tile selection and feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tile in sorted(df_all['Tile'].unique()):\n",
    "    print(tile)\n",
    "\n",
    "for column in df_all.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_in = \"(0, 1, 'N16-244-2A')\"       # string for specific tile of interest. Make sure string perfectly matches\n",
    "feature = 'mean_intensity_PD-L1_nuc'    # string for specific feature of interest. Make sure string perfectly matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tile = df_all[df_all['Tile'] == tile_in].reset_index()\n",
    "\n",
    "coords = np.asarray(df_tile[['centroid-1', 'centroid-0']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import Delaunay, Voronoi, voronoi_plot_2d\n",
    "\n",
    "vor = Voronoi(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "fig = voronoi_plot_2d(vor, show_vertices = False, show_points = False)\n",
    "fig.set_size_inches(10,10)\n",
    "c = df_tile[feature]\n",
    "minima = min(c)\n",
    "maxima = max(c)\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=minima, vmax=maxima, clip=True)\n",
    "mapper = cm.ScalarMappable(norm=norm, cmap=cm.viridis)\n",
    "\n",
    "\n",
    "for r in range(len(vor.point_region)):\n",
    "    region = vor.regions[vor.point_region[r]]\n",
    "    if not -1 in region:\n",
    "        polygon = [vor.vertices[i] for i in region]\n",
    "        plt.fill(*zip(*polygon), color=mapper.to_rgba(c[r]))\n",
    "        \n",
    "plt.ylim(0,2048)\n",
    "plt.xlim(0, 2048)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.savefig(f'{plots_folder}/Vor_demo{feature}.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
