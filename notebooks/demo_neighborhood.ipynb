{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "from scipy.spatial import Delaunay, Voronoi, voronoi_plot_2d\n",
    "from grispy import GriSPy\n",
    "from glob import glob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering     \n",
    "from scipy import stats\n",
    "from scipy.stats import beta\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "from itertools import permutations, combinations\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.colors as mcolors\n",
    "from distinctipy import distinctipy\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from skimage import io, transform, util, img_as_float\n",
    "from skimage.filters import gaussian\n",
    "from skimage.color import separate_stains, hax_from_rgb\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.util import img_as_ubyte, crop\n",
    "from skimage.draw import rectangle\n",
    "from skimage.morphology import remove_small_objects, remove_small_holes\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "available_cores = multiprocessing.cpu_count()\n",
    "\n",
    "import phenograph\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from grispy import GriSPy\n",
    "\n",
    "import turtle\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "str_now = str(now)[:10]\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set pipeline output directory\n",
    "output_dir = '/home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/Python_Output'\n",
    "#set image input directory\n",
    "raw_img_dir = '/home/workstation/Dropbox (VU Basic Sciences)/2022-08-09 CASSATT Demo Dataset/RawImages'  # for each round of imaging, images should be saved by the slide name and placed in a folder named for the marker that has been stained in the contained images\n",
    "# input order of MxIHC staining\n",
    "stain_order = ['PD-1', 'PD-L1', 'CD68', 'FoxP3', 'Iba-1', 'CD8', 'CD4']\n",
    "# input patient IDs\n",
    "slide_id = ['N16-244-2A']   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting variables\n",
    "\n",
    "#BaseAligned image input\n",
    "input_folder = output_dir + '/BaseAligned'\n",
    "#Tiles output\n",
    "output_folder = output_dir + '/Tiled'\n",
    "# registered stack output\n",
    "reg_folder = output_dir + '/Registered'\n",
    "\n",
    "tile_x, tile_y = 2048, 2048              # Can Edit Tile Size Here\n",
    "overlap_x, overlap_y = 512, 512          # Can Edit Overlap Here\n",
    "\n",
    "background_int = 250                     # Can Edit Mean Pixel Intensity for Tissue Detection Here\n",
    "\n",
    "_ransacReprojThreshold = 3               # Can Edit Maximum allowed reprojection error to treat a point pair as an inlier (used in the RANSAC method only) for Keypoint registration\n",
    "\n",
    "_num_good = 10                           # Can Edit Minimum number of good matches needed for keypoint registration to be considered successful\n",
    "_averageDistance = 0.5                   # Can Edit Maximum average distance of keypoint matches for registration to be considered successful\n",
    "\n",
    "seed = 42                                # seed value to make results reproducable.  Set to 'None' to use a random seed and gain increased multiprocessing resulting in faster results \n",
    "\n",
    "# Phenotypic cluster identification\n",
    "manual_gated = False                      # set to 'True' if manually gating outside of CASSATT. set to 'False' to run automated cluster identification within CASSATT. \n",
    "cluster_on_original = True               # set to 'True' to cluster on original parameter space, set to 'False' to cluster on dimensionality reduced space\n",
    "\n",
    "# Neighborhood Analysis Variables\n",
    "neighbor_search = 'voronoi'              # either 'voronoi', 'shell', or 'knn'\n",
    "neighbor_search_value = 10               # if neighbor_search is set to 'voronoi', disreguard. if set to 'shell' value = maximum pixel distance for neighbor cell detection. if set to 'knn' value = number of neighbors to be found\n",
    "n_neighbor_clusters = 15                 # number of neighbor clusters to find with kmeans clustering of index cell neighbor fractions\n",
    "\n",
    "# Parent folder for neighborhood analysis files\n",
    "neighborhood_dir = output_dir + '/Neighborhood'\n",
    "\n",
    "# Individual files for each slide / gate combination.  Naming scheme should be as follows : SlideID_GateID.csv  (Must be underscore delimited)\n",
    "gates_dir = neighborhood_dir + '/Gated_Input'\n",
    "\n",
    "# Individual csvs per slide\n",
    "perslide_dir = neighborhood_dir + '/Gated_Perslide'\n",
    "\n",
    "# Log Odds outputs\n",
    "logodds_dir = neighborhood_dir + '/LogOdds'\n",
    "\n",
    "#Interaction Counts\n",
    "interactions_dir = logodds_dir + '/Interactions'\n",
    "\n",
    "# Neighbor cell csvs\n",
    "neighbor_csv_dir = neighborhood_dir + '/Neighbor_CSVs'\n",
    "\n",
    "# Neighbor cell csvs + umap coordinates based on neighbor frequencies\n",
    "neighbor_umap_dir = neighborhood_dir + '/Neighborhood_Umap_CSVs'\n",
    "\n",
    "# Neighborhood analysis plot outputs\n",
    "neighbor_plots_dir = neighborhood_dir + '/Neighborhood_Plots'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(neighborhood_dir, exist_ok = True)\n",
    "os.makedirs(gates_dir, exist_ok = True)\n",
    "os.makedirs(perslide_dir, exist_ok = True)\n",
    "os.makedirs(logodds_dir, exist_ok = True)\n",
    "os.makedirs(neighbor_csv_dir, exist_ok = True)\n",
    "os.makedirs(neighbor_umap_dir, exist_ok = True)\n",
    "os.makedirs(interactions_dir, exist_ok = True)\n",
    "os.makedirs(neighbor_plots_dir, exist_ok = True)\n",
    "os.makedirs(neighbor_plots_dir + '/Per_Slide', exist_ok = True)\n",
    "os.makedirs(neighbor_plots_dir + '/Per_Cluster', exist_ok = True)                # UPDATED\n",
    "os.makedirs(interactions_dir + '/Slide', exist_ok = True)\n",
    "os.makedirs(interactions_dir + '/Tile', exist_ok = True)\n",
    "os.makedirs(neighbor_plots_dir + '/MergedSlides', exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Gated Population Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if manual_gated == True:\n",
    "    l_csv = sorted(glob(gates_dir + '/*.csv'))    # read in all manually gated populations\n",
    "    \n",
    "    l_pops = []\n",
    "    l_slides = []\n",
    "    l_data = []\n",
    "\n",
    "    for file in l_csv:\n",
    "        name = os.path.basename(file)\n",
    "        pat, pop = os.path.splitext(name)[0].split('_')\n",
    "\n",
    "        data = pd.read_csv(file, header = 0)\n",
    "        data['pop_ID'] = pop\n",
    "        data['slide_id'] = pat\n",
    "        data['Tile'] = data.apply(lambda row: (int(row.Tile_row), int(row.Tile_col)), axis = 1)\n",
    "        l_pops.append(pop)\n",
    "        l_slides.append(pat)\n",
    "        l_data.append(data)\n",
    "\n",
    "    df_data = pd.concat(l_data)   \n",
    "    l_pops = sorted(list(set(l_pops)))\n",
    "    l_slides = sorted(list(set(l_slides)))\n",
    "    \n",
    "    vor_colors = ['#ff0000',\n",
    "              '#cccc66', \n",
    "              '#ffff00', \n",
    "              '#ff9999',\n",
    "              '#99cc00',\n",
    "              '#ff6600',\n",
    "              '#009900',\n",
    "              '#ffcc66', \n",
    "              '#cc66cc',\n",
    "              '#00cccc',\n",
    "              '#ff66cc',\n",
    "              '#0000ff',\n",
    "              '#663399', \n",
    "              '#cccccc']\n",
    "\n",
    "    vor_pops = {}\n",
    "    for index, l in enumerate(l_pops):\n",
    "        vor_pops[l] = vor_colors[index]\n",
    "\n",
    "    #confirm populations and hex codes match as desired    \n",
    "    vor_pops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Phenotypic Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataframe of all cells and all cell features\n",
    "if manual_gated == False:\n",
    "    df_all = pd.read_csv(output_dir + '/df_all.csv')   # load all features.\n",
    "    for index, x in enumerate(list(df_all.columns)):\n",
    "        print(index, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if manual_gated == False:\n",
    "    # choose column indices to use for dimentionality reduction and/or clustering \n",
    "    clustering_features = [8, 12, 13, 15, 17, 20, 21, 23]  # USER EDIT HERE, choose indices from printout above\n",
    "\n",
    "    clustering_features = [list(df_all.columns)[y] for y in clustering_features] # lists column names of selected \n",
    "\n",
    "    \n",
    "    \n",
    "    reducer = umap.UMAP(n_neighbors = 50, min_dist = 0.1, random_state = seed)\n",
    "\n",
    "    embedding = reducer.fit_transform(df_all.filter(items = clustering_features, axis = 1))\n",
    "\n",
    "    df_embedding = pd.DataFrame(data = embedding, columns = ['pheno_umap1', 'pheno_umap2'])\n",
    "\n",
    "    df_all = df_all.reset_index(drop = True).join(df_embedding)\n",
    "\n",
    "    # cluster identification\n",
    "    if cluster_on_original == True: \n",
    "        communities, graph, Q = phenograph.cluster(df_all.filter(items = clustering_features, axis = 1))\n",
    "        df_all['pop_ID'] = communities\n",
    "        df_all['pop_ID']= df_all['pop_ID'].apply(str).str.zfill(2)\n",
    "        df_all['pop_ID'] = 'phenocluster-' + df_all['pop_ID']\n",
    "    else: \n",
    "        communities, graph, Q = phenograph.cluster(df_all[['pheno_umap1', 'pheno_umap2']])\n",
    "        df_all['pop_ID'] = communities\n",
    "        df_all['pop_ID'] = df_all['pop_ID'].apply(str).str.zfill(2)\n",
    "        df_all['pop_ID'] = 'phenocluster-' + df_all['pop_ID']\n",
    "\n",
    "\n",
    "    df_data = df_all\n",
    "    l_pops = sorted(df_data['pop_ID'].unique())\n",
    "    df_data['slide_id'] = df_all['Tile'].apply(lambda x: eval(x)[2])\n",
    "    l_slides = sorted(df_data['slide_id'].unique())\n",
    "    df_data['Tile'] = df_all['Tile'].apply(lambda x: (eval(x)[0], eval(x)[1]))\n",
    "    \n",
    "    cmap = distinctipy.get_colors(len(l_pops))\n",
    "    vor_colors = [mcolors.rgb2hex(x) for x in cmap]\n",
    "    vor_pops = {}\n",
    "    for index, l in enumerate(l_pops):\n",
    "        vor_pops[l] = vor_colors[index]\n",
    "    [print(x) for x in vor_pops.items()]\n",
    "    \n",
    "    #confirm populations and hex codes match as desired    \n",
    "    [print(x) for x in vor_pops.items()]\n",
    "    plt.figure(figsize=(15,15))\n",
    "    sns.scatterplot(data = df_all, x = 'pheno_umap1', y = 'pheno_umap2', hue = 'pop_ID', palette = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "sns.scatterplot(data = df_all, x = 'pheno_umap1', y = 'pheno_umap2', hue = 'pop_ID', palette = 'tab20', linewidth = 0)\n",
    "plt.savefig('/home/workstation/Dropbox (VU Basic Sciences)/2021-10-05 MxIHC Manuscript Figure Images/2022-11 CASSATT MS and Figures Reviewer Response and Updates/Figures/Lovly Figure Files/clusters_on_orig.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Log Odds Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates global percentages of each gated population across all slides\n",
    "d_pops = {}\n",
    "for i in l_pops:\n",
    "    global_ratio = df_data[df_data.pop_ID == i].shape[0]/df_data.shape[0]\n",
    "    d_pops[i] = global_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resaves gated cells per slide\n",
    "for i, g in df_data.groupby('slide_id'):\n",
    "    g.to_csv(perslide_dir + '/{}.csv'.format(i), index = False)\n",
    "\n",
    "l_intfiles = sorted(glob(perslide_dir + '/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_niche_slide(csv):\n",
    "    log = open(logodds_dir + '/calc_niche_slide_savelog.txt', 'w+')\n",
    "    raw_int = pd.read_csv(csv)\n",
    "    for pop in l_pops:\n",
    "        raw_int[pop] = raw_int.apply(lambda row: 1 if row.pop_ID == pop else 0 , axis = 1)\n",
    "    all_long_list = []\n",
    "    total_count = 0\n",
    "    for tile in raw_int.Tile.unique():\n",
    "        try:\n",
    "            df_tile = raw_int[raw_int['Tile']==tile]\n",
    "            df_tile.reset_index(inplace=True)\n",
    "            slide_id = os.path.splitext(os.path.basename(csv))[0]\n",
    "            tile_r = int(df_tile.Tile_row[0])\n",
    "            tile_c = int(df_tile.Tile_col[0])\n",
    "            # define new array of just coordinates to find first order neighbors\n",
    "            coords = np.asarray(df_tile[['centroid-1', 'centroid-0']])\n",
    "            # create dict of indexes of cells : neighbor cells\n",
    "            if neighbor_search == 'voronoi':\n",
    "                tri = Delaunay(coords)\n",
    "                vor = Voronoi(coords)\n",
    "                _neighbors = defaultdict(set)\n",
    "                indptr, indices = tri.vertex_neighbor_vertices\n",
    "                for i in range(len(coords)):\n",
    "                    i_neigh = indices[indptr[i]:indptr[i+1]]\n",
    "                    _neighbors[i] = set(i_neigh)\n",
    "\n",
    "                total_count += len(vor.ridge_vertices)\n",
    "            elif neighbor_search == 'shell':\n",
    "                gsp = GriSPy(coords)\n",
    "                upper_radii = float(neighbor_search_value)\n",
    "                lower_radii = 0.01\n",
    "                shell_dist, shell_ind = gsp.shell_neighbors(\n",
    "                    coords,\n",
    "                    distance_lower_bound=lower_radii,\n",
    "                    distance_upper_bound=upper_radii\n",
    "                )\n",
    "\n",
    "                _neighbors = {}\n",
    "                count = 0\n",
    "                for index, v in enumerate(shell_ind):\n",
    "                    _neighbors[index] = set(v)\n",
    "                    count += len(v)\n",
    "                total_count += count\n",
    "            elif neighbor_search == 'knn':\n",
    "                gsp = GriSPy(coords)\n",
    "                knn = int(neighbor_search_value)\n",
    "                knn_dist, knn_ind = gsp.nearest_neighbors(\n",
    "                    coords,\n",
    "                    n = knn\n",
    "                )\n",
    "                _neighbors = {}\n",
    "                count = 0\n",
    "                for index, v in enumerate(knn_ind):\n",
    "                    _neighbors[index] = set(v)\n",
    "                    count += len(v)\n",
    "                total_count += count\n",
    "                    \n",
    "            long_list = []\n",
    "            for k, v in _neighbors.items():\n",
    "                for value in list(v):\n",
    "                    long_list.append([k, value])\n",
    "            if len(long_list) > 0:\n",
    "                df_long_list = pd.DataFrame(long_list, columns = ['index_cell', 'neighbor'])\n",
    "                df_long_list['sorted_row'] = [sorted([a,b]) for a, b in zip(df_long_list.index_cell, df_long_list.neighbor)]\n",
    "                df_long_list['sorted_row'] = df_long_list['sorted_row'].astype(str)\n",
    "                df_long_list.drop_duplicates(subset = ['sorted_row'], inplace = True)\n",
    "                df_long_list.reset_index(inplace = True)  \n",
    "                df_long_list['index_id'] = df_long_list.apply(lambda row: list(df_tile.loc[[row.index_cell]]['pop_ID'])[0], axis = 1)\n",
    "                df_long_list['neighbor_id'] = df_long_list.apply(lambda row: list(df_tile.loc[[row.neighbor]]['pop_ID'])[0], axis = 1)\n",
    "                df_long_list['sorted_row2'] = [sorted([a,b]) for a,b in zip(df_long_list.index_id, df_long_list.neighbor_id)]\n",
    "                df_long_list['sorted_row2'] = df_long_list['sorted_row2'].astype(str)\n",
    "\n",
    "                all_long_list.append(df_long_list)\n",
    "\n",
    "                df_iniche = pd.DataFrame()\n",
    "\n",
    "                for k, v in _neighbors.items():\n",
    "                    if v == set():\n",
    "                        continue\n",
    "                    else:\n",
    "                        _df_iniche = pd.DataFrame()\n",
    "                        for x in range(len(_neighbors[k])):\n",
    "                            _df_iniche = _df_iniche.append(df_tile.loc[[list(v)[x]]])\n",
    "                            _df_iniche.drop(columns = ['index', 'Tile', 'slide_id', 'pop_ID'], inplace = True)\n",
    "\n",
    "\n",
    "                        sums = pd.DataFrame(_df_iniche.sum(axis = 0)/len(_df_iniche.index)).transpose() \n",
    "                        sums.drop(sums.columns.difference(l_pops), 1, inplace = True)\n",
    "                        sums.reset_index(inplace = True)\n",
    "\n",
    "                        df_index_cell = pd.DataFrame(df_tile.loc[k][['centroid-0', 'centroid-1']]).transpose()\n",
    "                        df_index_cell.reset_index(inplace = True)\n",
    "\n",
    "                        df_niche = pd.concat([sums, df_index_cell], axis = 1)\n",
    "                        df_iniche = df_iniche.append(df_niche)\n",
    "\n",
    "                df_merge = df_tile.drop(columns = [x for x in l_pops])\n",
    "                df_merge.drop(columns = ['index'], inplace = True)\n",
    "                new_df = df_iniche.merge(df_merge, on = ['centroid-0', 'centroid-1'])\n",
    "                new_df.drop(columns = new_df.columns[0], axis = 1, inplace = True)\n",
    "                # For each cell neighbor-frequencies of each gated population\n",
    "                new_df.to_csv(neighbor_csv_dir + '/{}_r{}_c{}.csv'.format(slide_id, tile_r, tile_c), index = False)\n",
    "        except:\n",
    "            log.write('Error on {} {} {}\\n'.format(slide_id, tile_r, tile_c))\n",
    "            pass\n",
    "        \n",
    "    df_long_list = pd.concat(all_long_list)\n",
    "    pop_counts = {}\n",
    "    for pop in l_pops:\n",
    "        pop_counts[pop] = len(df_long_list[(df_long_list['index_id']==pop) | (df_long_list['neighbor_id'] == pop)].index)\n",
    "\n",
    "\n",
    "    interactions = df_long_list.groupby(['sorted_row2']).size().reset_index(name='interaction_counts')\n",
    "\n",
    "    interactions['beta_dist'] = interactions.apply(lambda row: beta.stats(row.interaction_counts, total_count-row.interaction_counts, moments = 'm'), axis = 1)\n",
    "\n",
    "    interactions['pops'] = interactions.apply(lambda row: ast.literal_eval(row.sorted_row2), axis = 1)\n",
    "    interactions['popA'] = interactions.apply(lambda row: row.pops[0], axis = 1)\n",
    "    interactions['popB'] = interactions.apply(lambda row: row.pops[1], axis = 1)\n",
    "\n",
    "    interactions['popA_freq'] = interactions.apply(lambda row: pop_counts[row.popA], axis = 1)\n",
    "    interactions['popB_freq'] = interactions.apply(lambda row: pop_counts[row.popB], axis = 1)\n",
    "    interactions['total_count'] = total_count\n",
    "\n",
    "    interactions['odds_ratio'] = interactions.apply(lambda row: row.beta_dist/ ((row.popA_freq/total_count) * (row.popB_freq/total_count)), axis = 1)\n",
    "    interactions['log_odds_ratio'] = interactions.apply(lambda row: np.log(row.odds_ratio), axis = 1)\n",
    "    interactions = interactions[['pops', 'popA', 'popB', 'interaction_counts', 'beta_dist', 'popA_freq', 'popB_freq', 'total_count', 'odds_ratio', 'log_odds_ratio','sorted_row2']]\n",
    "    #log odds of each gated population versus every other gated population across full slide\n",
    "    interactions.to_csv(interactions_dir + '/Slide/interactions_{}.csv'.format(os.path.splitext(os.path.basename(csv))[0]))\n",
    "    \n",
    "    log.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_niche_tiles(csv):\n",
    "    # Calculate log odds of gated populations within each tile\n",
    "    log = open(logodds_dir + '/calc_niche_tiles_savelog.txt', 'w+')\n",
    "    raw_int = pd.read_csv(csv)\n",
    "    for pop in l_pops:\n",
    "        raw_int[pop] = raw_int.apply(lambda row: 1 if row.pop_ID == pop else 0 , axis = 1)\n",
    "    for tile in raw_int.Tile.unique():\n",
    "        total_count = 0\n",
    "        try:\n",
    "            df_tile = raw_int[raw_int['Tile']==tile]\n",
    "            df_tile.reset_index(inplace=True)\n",
    "            slide_id = os.path.splitext(os.path.basename(csv))[0]\n",
    "            tile_r = int(df_tile.Tile_row[0])\n",
    "            tile_c = int(df_tile.Tile_col[0])\n",
    "            # define new array of just coordinates to find first order neighbors\n",
    "            coords = np.asarray(df_tile[['centroid-1', 'centroid-0']])\n",
    "            # create dict of indexes of cells : neighbor cells\n",
    "            if neighbor_search == 'voronoi':\n",
    "                tri = Delaunay(coords)\n",
    "                vor = Voronoi(coords)\n",
    "                _neighbors = defaultdict(set)\n",
    "                indptr, indices = tri.vertex_neighbor_vertices\n",
    "                for i in range(len(coords)):\n",
    "                    i_neigh = indices[indptr[i]:indptr[i+1]]\n",
    "                    _neighbors[i] = set(i_neigh)\n",
    "\n",
    "                total_count += len(vor.ridge_vertices)\n",
    "            elif neighbor_search == 'shell':\n",
    "                gsp = GriSPy(coords)\n",
    "                upper_radii = float(neighbor_search_value)\n",
    "                lower_radii = 0.01\n",
    "                shell_dist, shell_ind = gsp.shell_neighbors(\n",
    "                    coords,\n",
    "                    distance_lower_bound=lower_radii,\n",
    "                    distance_upper_bound=upper_radii\n",
    "                )\n",
    "\n",
    "                _neighbors = {}\n",
    "                count = 0\n",
    "                for index, v in enumerate(shell_ind):\n",
    "                    _neighbors[index] = set(v)\n",
    "                    count += len(v)\n",
    "                total_count += count\n",
    "            elif neighbor_search == 'knn':\n",
    "                gsp = GriSPy(coords)\n",
    "                knn = int(neighbor_search_value)\n",
    "                knn_dist, knn_ind = gsp.nearest_neighbors(\n",
    "                    coords,\n",
    "                    n = knn\n",
    "                )\n",
    "                _neighbors = {}\n",
    "                count = 0\n",
    "                for index, v in enumerate(knn_ind):\n",
    "                    _neighbors[index] = set(v)\n",
    "                    count += len(v)\n",
    "                total_count += count\n",
    "            long_list = []\n",
    "            for k, v in _neighbors.items():\n",
    "                for value in list(v):\n",
    "                    long_list.append([k, value])\n",
    "            if len(long_list) > 0:\n",
    "                df_long_list = pd.DataFrame(long_list, columns = ['index_cell', 'neighbor'])\n",
    "                df_long_list['sorted_row'] = [sorted([a,b]) for a, b in zip(df_long_list.index_cell, df_long_list.neighbor)]\n",
    "                df_long_list['sorted_row'] = df_long_list['sorted_row'].astype(str)\n",
    "                df_long_list.drop_duplicates(subset = ['sorted_row'], inplace = True)\n",
    "                df_long_list.reset_index(inplace = True)  \n",
    "                df_long_list['index_id'] = df_long_list.apply(lambda row: list(df_tile.loc[[row.index_cell]]['pop_ID'])[0], axis = 1)\n",
    "                df_long_list['neighbor_id'] = df_long_list.apply(lambda row: list(df_tile.loc[[row.neighbor]]['pop_ID'])[0], axis = 1)\n",
    "                df_long_list['sorted_row2'] = [sorted([a,b]) for a,b in zip(df_long_list.index_id, df_long_list.neighbor_id)]\n",
    "                df_long_list['sorted_row2'] = df_long_list['sorted_row2'].astype(str)\n",
    "        \n",
    "            pop_counts = {}\n",
    "            for pop in l_pops:\n",
    "                pop_counts[pop] = len(df_long_list[(df_long_list['index_id']==pop) | (df_long_list['neighbor_id'] == pop)].index)\n",
    "\n",
    "\n",
    "            interactions = df_long_list.groupby(['sorted_row2']).size().reset_index(name='interaction_counts')\n",
    "\n",
    "            interactions['beta_dist'] = interactions.apply(lambda row: beta.stats(row.interaction_counts, total_count-row.interaction_counts, moments = 'm'), axis = 1)\n",
    "\n",
    "            interactions['pops'] = interactions.apply(lambda row: ast.literal_eval(row.sorted_row2), axis = 1)\n",
    "            interactions['popA'] = interactions.apply(lambda row: row.pops[0], axis = 1)\n",
    "            interactions['popB'] = interactions.apply(lambda row: row.pops[1], axis = 1)\n",
    "\n",
    "            interactions['popA_freq'] = interactions.apply(lambda row: pop_counts[row.popA], axis = 1)\n",
    "            interactions['popB_freq'] = interactions.apply(lambda row: pop_counts[row.popB], axis = 1)\n",
    "            interactions['total_count'] = total_count\n",
    "\n",
    "            interactions['odds_ratio'] = interactions.apply(lambda row: row.beta_dist/ ((row.popA_freq/total_count) * (row.popB_freq/total_count)), axis = 1)\n",
    "            interactions['log_odds_ratio'] = interactions.apply(lambda row: np.log(row.odds_ratio), axis = 1)\n",
    "            interactions = interactions[['pops', 'popA', 'popB', 'interaction_counts', 'beta_dist', 'popA_freq', 'popB_freq', 'total_count', 'odds_ratio', 'log_odds_ratio','sorted_row2']]\n",
    "            interactions.to_csv(interactions_dir + '/Tile/interactions_{}_r{}_c{}.csv'.format(os.path.splitext(os.path.basename(csv))[0], tile_r, tile_c))\n",
    "        except:\n",
    "            log.write('Error on {}_{}_{}\\n'.format(slide_id, tile_r, tile_c))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Parallel processing of neighborhood niche analysis per slide\n",
    "#Can be time + resource intensive on large datasets, can edit n_jobs to 1 to disable parallel processing if workstation is running out of memory\n",
    "Parallel(n_jobs = int(available_cores * 0.9), backend = 'loky')(delayed(calc_niche_slide)(x)for x in tqdm(l_intfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel processing of neighborhood niche analysis per tile\n",
    "Parallel(n_jobs = int(available_cores * 0.9), backend = 'loky')(delayed(calc_niche_tiles)(x)for x in tqdm(l_intfiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Slide and All Slides Log Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate log odds across all slides\n",
    "all_long_list = []\n",
    "total_count = 0\n",
    "for csv in l_intfiles:\n",
    "    raw_int = pd.read_csv(csv)\n",
    "    for pop in l_pops:\n",
    "        raw_int[pop] = raw_int.apply(lambda row: 1 if row.pop_ID == pop else 0 , axis = 1)\n",
    "    for tile in raw_int.Tile.unique():\n",
    "        try:\n",
    "            df_tile = raw_int[raw_int['Tile']==tile]\n",
    "            df_tile.reset_index(inplace=True)\n",
    "            slide_id = os.path.basename(csv)[:-4]\n",
    "            tile_r = int(df_tile.Tile_row[0])\n",
    "            tile_c = int(df_tile.Tile_col[0])\n",
    "            # define new array of just coordinates to find first order neighbors\n",
    "            coords = np.asarray(df_tile[['centroid-1', 'centroid-0']])\n",
    "            # create dict of indexes of cells : neighbor cells\n",
    "            tri = Delaunay(coords)\n",
    "            vor = Voronoi(coords)\n",
    "            _neighbors = defaultdict(set)\n",
    "            indptr, indices = tri.vertex_neighbor_vertices\n",
    "            for i in range(len(coords)):\n",
    "                i_neigh = indices[indptr[i]:indptr[i+1]]\n",
    "                _neighbors[i] = set(i_neigh)\n",
    "\n",
    "            total_count += len(vor.ridge_vertices)\n",
    "\n",
    "            long_list = []\n",
    "            for k, v in _neighbors.items():\n",
    "                for value in list(v):\n",
    "                    long_list.append([k, value])\n",
    "\n",
    "            df_long_list = pd.DataFrame(long_list, columns = ['index_cell', 'neighbor'])\n",
    "            df_long_list['sorted_row'] = [sorted([a,b]) for a, b in zip(df_long_list.index_cell, df_long_list.neighbor)]\n",
    "            df_long_list['sorted_row'] = df_long_list['sorted_row'].astype(str)\n",
    "            df_long_list.drop_duplicates(subset = ['sorted_row'], inplace = True)\n",
    "            df_long_list.reset_index(inplace = True)  \n",
    "            df_long_list['index_id'] = df_long_list.apply(lambda row: list(df_tile.loc[[row.index_cell]]['pop_ID'])[0], axis = 1)\n",
    "            df_long_list['neighbor_id'] = df_long_list.apply(lambda row: list(df_tile.loc[[row.neighbor]]['pop_ID'])[0], axis = 1)\n",
    "            df_long_list['sorted_row2'] = [sorted([a,b]) for a,b in zip(df_long_list.index_id, df_long_list.neighbor_id)]\n",
    "            df_long_list['sorted_row2'] = df_long_list['sorted_row2'].astype(str)\n",
    "\n",
    "            all_long_list.append(df_long_list)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "df_long_list = pd.concat(all_long_list)\n",
    "pop_counts = {}\n",
    "for pop in l_pops:\n",
    "    pop_counts[pop] = len(df_long_list[(df_long_list['index_id']==pop) | (df_long_list['neighbor_id'] == pop)].index)\n",
    "\n",
    "\n",
    "interactions = df_long_list.groupby(['sorted_row2']).size().reset_index(name='interaction_counts')\n",
    "\n",
    "interactions['beta_dist'] = interactions.apply(lambda row: beta.stats(row.interaction_counts, (total_count)-row.interaction_counts, moments = 'm'), axis = 1)\n",
    "\n",
    "interactions['pops'] = interactions.apply(lambda row: ast.literal_eval(row.sorted_row2), axis = 1)\n",
    "interactions['popA'] = interactions.apply(lambda row: row.pops[0], axis = 1)\n",
    "interactions['popB'] = interactions.apply(lambda row: row.pops[1], axis = 1)\n",
    "\n",
    "interactions['popA_freq'] = interactions.apply(lambda row: pop_counts[row.popA], axis = 1)\n",
    "interactions['popB_freq'] = interactions.apply(lambda row: pop_counts[row.popB], axis = 1)\n",
    "interactions['total_count'] = total_count\n",
    "\n",
    "interactions['odds_ratio'] = interactions.apply(lambda row: row.beta_dist/ ((row.popA_freq/total_count) * (row.popB_freq/total_count)), axis = 1)\n",
    "interactions['log_odds_ratio'] = interactions.apply(lambda row: np.log(row.odds_ratio), axis = 1)\n",
    "interactions = interactions[['pops', 'popA', 'popB', 'interaction_counts', 'beta_dist', 'popA_freq', 'popB_freq', 'total_count', 'odds_ratio', 'log_odds_ratio','sorted_row2']]\n",
    "interactions.to_csv(logodds_dir + '/all_slides_interactions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interactions = pd.read_csv(logodds_dir + '/all_slides_interactions.csv')\n",
    "halfheatmap = interactions[['popA', 'popB', 'log_odds_ratio']]\n",
    "halfheatmap2 = halfheatmap.rename(columns = {'popA' : 'popB', 'popB' : 'popA'})\n",
    "full_heatmap = pd.concat([halfheatmap, halfheatmap2]).drop_duplicates()\n",
    "heatmap2 = full_heatmap.pivot('popA', 'popB', 'log_odds_ratio')\n",
    "\n",
    "count_inter = interactions[['popA', 'popB', 'interaction_counts']]\n",
    "count_inter2 = count_inter.rename(columns = {'popA' : 'popB', 'popB' : 'popA'})\n",
    "full_count_inter = pd.concat([count_inter, count_inter2]).drop_duplicates()\n",
    "count_inter_heatmap = full_count_inter.pivot('popA', 'popB', 'interaction_counts')\n",
    "\n",
    "rdbl = sns.diverging_palette(240,10, n = 99, l = 50, s=100,as_cmap = True)\n",
    "#divnorm = mcolors.TwoSlopeNorm(vmin = min(-0.2, heatmap2.min().min()), vcenter = 0, vmax = max(heatmap2.max().max(), 0.2))\n",
    "divnorm = mcolors.TwoSlopeNorm(vmin = -2, vmax = 6, vcenter = 0)\n",
    "g = sns.clustermap(heatmap2.fillna(0), cmap = rdbl, norm = divnorm, method = 'average', annot = count_inter_heatmap, annot_kws={\"fontsize\":5})#.fig.suptitle('Log Odds Ratio of Population Interactions')\n",
    "plt.savefig(logodds_dir + '/Log_Odds_Ratio_clustergram_all_slides.pdf')\n",
    "heatmap3 = heatmap2[[heatmap2.columns[i] for i in g.dendrogram_col.reordered_ind]]\n",
    "new_order = [col for col in heatmap3.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# per slide log odds interactions\n",
    "l_interactions_slide = sorted(glob(interactions_dir + '/Slide/*.csv'))\n",
    "l_heatmaps = []\n",
    "for l in l_interactions_slide:\n",
    "    plt.figure(figsize = (5,5))\n",
    "    interactions = pd.read_csv(l)\n",
    "    halfheatmap = interactions[['popA', 'popB', 'log_odds_ratio']]\n",
    "    halfheatmap2 = halfheatmap.rename(columns = {'popA' : 'popB', 'popB' : 'popA'})\n",
    "    full_heatmap = pd.concat([halfheatmap, halfheatmap2]).drop_duplicates()\n",
    "    heatmap2 = full_heatmap.pivot('popA', 'popB', 'log_odds_ratio')\n",
    "    heatmap3 = heatmap2.reindex(index = new_order, columns = new_order)\n",
    "    \n",
    "    count_inter = interactions[['popA', 'popB', 'interaction_counts']]\n",
    "    count_inter2 = count_inter.rename(columns = {'popA' : 'popB', 'popB' : 'popA'})\n",
    "    full_count_inter = pd.concat([count_inter, count_inter2]).drop_duplicates()\n",
    "    count_inter_heatmap = full_count_inter.pivot('popA', 'popB', 'interaction_counts')\n",
    "    count_inter_heatmap = count_inter_heatmap.reindex(index = new_order, columns = new_order)\n",
    "    \n",
    "    rdbl = sns.diverging_palette(240,10, n = 99, l = 50, s=100,as_cmap = True)\n",
    "    divnorm = mcolors.TwoSlopeNorm(vmin = -2, vcenter = 0, vmax = 6)\n",
    "    mask = heatmap3.isnull()\n",
    "    g = sns.heatmap(heatmap3, square = True, cmap = rdbl, norm = divnorm, cbar = True, mask = mask, \n",
    "#                    annot = count_inter_heatmap, annot_kws={\"fontsize\":3}, \n",
    "                    cbar_kws={'shrink':0.75})\n",
    "    g.set_title('Log Odds Ratio of Population Interactions {}'.format(os.path.basename(l)[13:-4]))\n",
    "    g.set_facecolor('#d8dcd6')\n",
    "    plt.savefig(logodds_dir + '/Log_Odds_Ratio_clustergram_{}.pdf'.format(os.path.splitext(os.path.basename(l))[0]), bbox_inches = 'tight')\n",
    "    l_heatmaps.append(heatmap3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all patient specific heatmaps and calculate median value for each heatmap square\n",
    "concat_heatmaps = pd.concat(l_heatmaps)\n",
    "concat_heatmaps = concat_heatmaps.groupby(level = 0).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of all median patient values \n",
    "plt.figure(figsize = (10,10))\n",
    "concat_heatmaps = concat_heatmaps.reindex(index = new_order, columns = new_order)\n",
    "mask = concat_heatmaps.isnull()\n",
    "g = sns.heatmap(concat_heatmaps, square = True, cmap = rdbl, norm = divnorm, cbar = True, mask = mask, cbar_kws={'shrink':0.75})\n",
    "g.set_facecolor('#d8dcd6')\n",
    "plt.title('Median Log Odds Ratio Across All Slides')\n",
    "plt.tight_layout()\n",
    "plt.savefig(logodds_dir + '/Median_Log_Odds_Ratio_All_Slides.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Per Tile Log Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_interactions_tile = sorted(glob(interactions_dir + '/Tile/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l_tile_heatmaps = []\n",
    "for pat in l_slides:\n",
    "    meta_tile = {}\n",
    "    for l in l_interactions_tile:\n",
    "        _,patid, r, c = os.path.basename(l)[:-4].split('_')\n",
    "        row = int(r[1:])\n",
    "        col = int(c[1:])\n",
    "        if patid == pat:\n",
    "            meta_tile[(row, col)] = l\n",
    "\n",
    "    l_tiles = [item for item in list(meta_tile.items())]\n",
    "    nrows = list(range(max([sublist[0][0] for sublist in l_tiles])+1))\n",
    "    ncols = list(range(max([sublist[0][1] for sublist in l_tiles])+1))\n",
    "    combined = [(f,s) for f in nrows for s in ncols]\n",
    "\n",
    "    l_for_fig = []\n",
    "    for item in combined:\n",
    "        try:\n",
    "            l_for_fig.append([item,meta_tile[item]])\n",
    "        except:\n",
    "            l_for_fig.append([item, 'blank'])\n",
    "\n",
    "    fig, axes = plt.subplots(nrows = max(nrows)+1, ncols = max(ncols)+1,figsize = (max(ncols*50), max(nrows*50)), sharex=True, sharey = True) \n",
    "    fig.subplots_adjust(hspace = 0.2, wspace = 0.2)\n",
    "    for tile in l_for_fig:\n",
    "        row = tile[0][0]\n",
    "        col = tile[0][1]\n",
    "        if tile[1] == 'blank':\n",
    "            axes[row, col].axis('off')\n",
    "        else:\n",
    "            interactions = pd.read_csv(tile[1])\n",
    "            halfheatmap = interactions[['popA', 'popB', 'log_odds_ratio']]\n",
    "            halfheatmap2 = halfheatmap.rename(columns = {'popA' : 'popB', 'popB' : 'popA'})\n",
    "            full_heatmap = pd.concat([halfheatmap, halfheatmap2]).drop_duplicates()\n",
    "            heatmap2 = full_heatmap.pivot('popA', 'popB', 'log_odds_ratio')\n",
    "            heatmap3 = heatmap2.reindex(index = new_order, columns = new_order)\n",
    "            \n",
    "            count_inter = interactions[['popA', 'popB', 'interaction_counts']]\n",
    "            count_inter2 = count_inter.rename(columns = {'popA' : 'popB', 'popB' : 'popA'})\n",
    "            full_count_inter = pd.concat([count_inter, count_inter2]).drop_duplicates()\n",
    "            count_inter_heatmap = full_count_inter.pivot('popA', 'popB', 'interaction_counts')\n",
    "            count_inter_heatmap = count_inter_heatmap.reindex(index = new_order, columns = new_order)\n",
    "            \n",
    "            rdbl = sns.diverging_palette(240,10, n = 99, l = 50, s=100,as_cmap = True)\n",
    "            divnorm = mcolors.TwoSlopeNorm(vmin = -2, vcenter = 0, vmax = 6)\n",
    "            mask = heatmap3.isnull()\n",
    "            g= sns.heatmap(heatmap3, ax = axes[row, col],xticklabels = False, yticklabels = False, square = True, mask = mask, cmap = rdbl, norm = divnorm, cbar = False, \n",
    "#                           annot = count_inter_heatmap, annot_kws={\"fontsize\":1}\n",
    "                          )\n",
    "            g.set(xlabel = None, ylabel = None)\n",
    "#            g.set_title('{}, {}'.format(row, col), fontsize = 5)\n",
    "            g.set_aspect('equal')\n",
    "            g.set_facecolor('#d8dcd6')\n",
    "            l_tile_heatmaps.append(heatmap3)\n",
    "    plt.suptitle('{}'.format(pat))\n",
    "#    plt.tight_layout(pad = 1)\n",
    "    plt.savefig(logodds_dir + '/Log_Odds_Ratio_clustergrams_{}_map.pdf'.format(pat), bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# interactions correlations\n",
    "l_corr = []\n",
    "for df in l_tile_heatmaps:\n",
    "    l_corr.append(df.to_numpy().flatten().tolist())\n",
    "df_corr = pd.DataFrame(l_corr)\n",
    "\n",
    "corrMatrix = df_corr.corr().dropna(axis = 1, how = 'all').dropna(axis = 0, how  = 'all')\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(corrMatrix, cmap = 'vlag')\n",
    "plt.savefig(logodds_dir + '/Correlation_Heatmap.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l_corr = []\n",
    "rdbl = sns.diverging_palette(240,10, n = 99, l = 50, s=100,as_cmap = True)\n",
    "divnorm = mcolors.TwoSlopeNorm(vmin = -1, vcenter = 0, vmax = 1)\n",
    "for i in range(len(l_pops)):\n",
    "    for df in l_tile_heatmaps:\n",
    "        l_corr.append(df.iloc[i])\n",
    "    df_corr = pd.DataFrame(l_corr)\n",
    "\n",
    "    corrMatrix = df_corr.corr().dropna(axis = 1, how = 'all').dropna(axis = 0, how  = 'all')\n",
    "    plt.figure(figsize = (10,10))\n",
    "    g = sns.heatmap(corrMatrix,cmap = rdbl, norm = divnorm)\n",
    "    g.set_title('Correlation Matrix {}'.format(new_order[i]))\n",
    "    g.set(xlabel = '{}'.format(new_order[i]), ylabel = '{}'.format(new_order[i]))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(logodds_dir + '/Correlation_Heatmap_{}.pdf'.format(new_order[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Log Odds on Specific Tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enter Slide_ID, Row_ID, and Col_ID of specific tile in the format: ['Slide_ID', 'Row_ID', 'Col_ID']\n",
    "tile_id = ['N14-248-1B', '10', '3']\n",
    "\n",
    "for l in l_interactions_tile:\n",
    "    _, slide, row, col = os.path.splitext(os.path.basename(l))[0].split('_')\n",
    "    r = row[1:]\n",
    "    c = col[1:]\n",
    "                                     \n",
    "    if [slide, r, c ] == tile_id:\n",
    "        interactions = pd.read_csv(l)\n",
    "        halfheatmap = interactions[['popA', 'popB', 'log_odds_ratio']]\n",
    "        halfheatmap2 = halfheatmap.rename(columns = {'popA' : 'popB', 'popB' : 'popA'})\n",
    "        full_heatmap = pd.concat([halfheatmap, halfheatmap2]).drop_duplicates()\n",
    "        heatmap2 = full_heatmap.pivot('popA', 'popB', 'log_odds_ratio')\n",
    "        heatmap3 = heatmap2.reindex(index = new_order, columns = new_order)\n",
    "\n",
    "        count_inter = interactions[['popA', 'popB', 'interaction_counts']]\n",
    "        count_inter2 = count_inter.rename(columns = {'popA' : 'popB', 'popB' : 'popA'})\n",
    "        full_count_inter = pd.concat([count_inter, count_inter2]).drop_duplicates()\n",
    "        count_inter_heatmap = full_count_inter.pivot('popA', 'popB', 'interaction_counts')\n",
    "        count_inter_heatmap = count_inter_heatmap.reindex(index = new_order, columns = new_order)\n",
    "        \n",
    "        rdbl = sns.diverging_palette(240,10, n = 99, l = 50, s=100,as_cmap = True)\n",
    "        divnorm = mcolors.TwoSlopeNorm(vmin = -2, vcenter = 0, vmax = 6)\n",
    "        plt.figure(figsize = (5,5))\n",
    "        g = sns.heatmap(heatmap3.fillna(0), square = True, cmap = rdbl, norm = divnorm, cbar = False).set_title('Log Odds Ratio of Population Interactions {}'.format(os.path.basename(l)[13:-4]))\n",
    "        plt.savefig(logodds_dir + '/Log_Odds_Ratio_clustergram_{}.pdf'.format(os.path.splitext(os.path.basename(l))[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Neighborhood Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all niche csvs.  Add global-x, global-y coordinates\n",
    "l_niche = []\n",
    "for root, dirs, files in os.walk(neighbor_csv_dir):\n",
    "    for i in files:\n",
    "        if i.endswith('.csv'):\n",
    "            l_niche.append(os.path.join(root, i))\n",
    "l_niche = sorted(l_niche)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Full Dataset neighborhood and feature data\n",
    "frames = []\n",
    "for i in l_niche:\n",
    "    k = pd.read_csv(i)\n",
    "    frames.append(k)\n",
    "\n",
    "df_full_dataset = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rigged for single slide\n",
    "slide_id = l_slides[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_single = df_full_dataset[df_full_dataset['slide_id'] == slide_id]\n",
    "\n",
    "pop_freq = {}\n",
    "for pop in l_pops:\n",
    "    pop_freq[pop] = df_single[pop].sum() / df_single.shape[0]\n",
    "\n",
    "drop_cols = l_pops + ['centroid-1', 'centroid-0']\n",
    "df_embed = df_single.drop(df_single.columns.difference(drop_cols), 1)\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors = 50, min_dist = 0.1, random_state = seed)\n",
    "\n",
    "embedding = reducer.fit_transform(df_embed.drop(['centroid-1', 'centroid-0'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x = embedding[:,0], y = embedding[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neighborhood - Per Slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighborSlide(current_slide):\n",
    "    df_single = df_full_dataset[df_full_dataset['slide_id'] == current_slide]\n",
    "\n",
    "    pop_freq = {}\n",
    "    for pop in l_pops:\n",
    "        pop_freq[pop] = df_single[pop].sum() / df_single.shape[0]*100\n",
    "\n",
    "    drop_cols = l_pops + ['centroid-1', 'centroid-0']\n",
    "    df_embed = df_single.drop(df_single.columns.difference(drop_cols), 1)\n",
    "    \n",
    "    reducer = umap.UMAP(n_neighbors = 50, min_dist = 0.1, random_state = seed)\n",
    "\n",
    "    embedding = reducer.fit_transform(df_embed.drop(['centroid-1', 'centroid-0'], axis = 1))\n",
    "\n",
    "    df_embedding = pd.DataFrame(data = embedding, columns = ['umap1', 'umap2'])\n",
    "\n",
    "    df_final = df_single.reset_index(drop = True).join(df_embedding)\n",
    "    \n",
    "    sorted_pops = sorted(pop_freq, key = pop_freq.get, reverse = True)\n",
    "    l_sorted_pops = []\n",
    "    for r in sorted_pops:\n",
    "        l_sorted_pops.append([r, pop_freq[r]])\n",
    "    df_sorted_pops = pd.DataFrame(l_sorted_pops, columns = ['pop_id', 'Percent of Total'])\n",
    "    \n",
    "    plt.figure(figsize = (5,5))\n",
    "    ax = sns.barplot(data = df_sorted_pops, x = 'pop_id', y = 'Percent of Total', palette = vor_pops)\n",
    "    plt.xticks(rotation = 90)\n",
    "    ax.set_yscale('log')\n",
    "    plt.ylim([0,100])\n",
    "    plt.title('Fraction of Total Cell Count per Population')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(neighbor_plots_dir + '/Per_Slide/{}_populations_fractions.pdf'.format(current_slide), dpi=500)\n",
    "    plt.close()\n",
    "    \n",
    "    for index, k in enumerate(pop_freq.keys()):\n",
    "        fig= plt.figure(figsize = (10,10))\n",
    "        if len(df_final) > 10000:\n",
    "            samplerows = df_final.sample(10000)\n",
    "        else:\n",
    "            samplerows = df_final\n",
    "        ax = sns.scatterplot(data = samplerows, x = 'umap1', y = 'umap2', hue = k, palette = 'viridis', linewidth = 0, s = 10)\n",
    "\n",
    "        plt.savefig(neighbor_plots_dir + '/Per_Slide/{}_umap_{}.pdf'.format(current_slide, k))\n",
    "        plt.close()\n",
    "        \n",
    "    n_clusters = n_neighbor_clusters\n",
    "    kmeans = KMeans(n_clusters = n_clusters, random_state = seed)\n",
    "    kmeans.fit(df_embedding)\n",
    "\n",
    "    y_kmeans = kmeans.predict(df_embedding)\n",
    "    \n",
    "    df_kmeans = pd.DataFrame(y_kmeans, columns = ['kmeans_cluster'])\n",
    "    df_final = df_final.join(df_kmeans)\n",
    "    \n",
    "    df_final.to_csv(neighbor_umap_dir + '/{}_umap.csv'.format(current_slide), index = False)\n",
    "\n",
    "\n",
    "    plt.figure(figsize = (5,5))\n",
    "    ax = sns.scatterplot(data = df_final, x = 'umap1', y = 'umap2', hue = 'kmeans_cluster', s = 3, linewidth = 0, palette = 'tab20')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc = 2, borderaxespad = 0.)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(neighbor_plots_dir + '/Per_Slide/{}_clusters_on_umap.pdf'.format(current_slide), dpi=500)\n",
    "    plt.close()\n",
    "    \n",
    "    cluster_cols = list(pop_freq.keys()).copy()\n",
    "    cluster_cols.append('kmeans_cluster')\n",
    "    df_clusters = df_final.drop(df_final.columns.difference(cluster_cols), 1)\n",
    "\n",
    "    df_clusters = df_clusters.groupby('kmeans_cluster').mean()  \n",
    "    \n",
    "    sns.clustermap(df_clusters, cmap = 'viridis')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(neighbor_plots_dir + '/Per_Slide/{}_clustergram.pdf'.format(current_slide), dpi=500)\n",
    "    plt.close()\n",
    "    \n",
    "    drop_cols = cluster_cols.copy()\n",
    "    drop_cols.extend(['centroid-0', 'centroid-1'])\n",
    "    cluster_homog = df_final[['centroid-0', 'centroid-1', 'kmeans_cluster']].merge(df_single.drop(df_single.columns.difference(drop_cols),1), on =['centroid-0', 'centroid-1'])\n",
    "\n",
    "    cluster_homog = cluster_homog.drop(columns = ['centroid-0', 'centroid-1'])\n",
    "    \n",
    "    df_homog = pd.DataFrame()\n",
    "    for i in range(len(np.unique(y_kmeans))):\n",
    "        out = pd.melt(cluster_homog[cluster_homog['kmeans_cluster']==i].drop(columns = 'kmeans_cluster')).groupby('variable').quantile(0.90).rename(columns = {'value' : 'kmeans_cluster_{}'.format(i)}).reset_index()\n",
    "        if i == 0:\n",
    "            df_homog = out\n",
    "        else:\n",
    "            df_homog = pd.merge(df_homog, out, on = 'variable')\n",
    "\n",
    "    order = df_homog.loc[(df_homog.iloc[:, 1:]!=0).any(axis = 1)]['variable'].tolist()[::-1]\n",
    "    \n",
    "    fig, axs = plt.subplots(int(n_clusters/4)+1,4, figsize = (20, int((n_clusters/4+1))*5))\n",
    "    fig.subplots_adjust(hspace=1,wspace=0.4)\n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
    "\n",
    "\n",
    "    for i in range(len(np.unique(y_kmeans))):\n",
    "        plt.subplot(4,int(n_clusters/4)+1, i+1)\n",
    "        g = sns.boxplot(data = pd.melt(cluster_homog[cluster_homog['kmeans_cluster']==i].drop(columns = 'kmeans_cluster')), x = 'variable', y = 'value', showfliers = False, order = order, palette = vor_pops)\n",
    "        g.set(ylim=(0,1.2))\n",
    "        g.tick_params(axis = 'x', labelrotation = 90)\n",
    "        g.set_xlabel('')\n",
    "        g.set_ylabel('Neighbor Frequency')\n",
    "        g.set_title('Cluster {}, n = {}'.format(str(i), len(cluster_homog[cluster_homog['kmeans_cluster']==i])))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(neighbor_plots_dir + '/Per_Slide/{}_Neighborhood_homogeneity_slide.pdf'.format(current_slide), dpi=500)\n",
    "    plt.close()   \n",
    "    \n",
    "    cluster_freq = {}                                                                                                           # UPDATED TO END\n",
    "    for cluster in sorted(cluster_homog['kmeans_cluster'].unique()):\n",
    "        cluster_freq[cluster] = cluster_homog[cluster_homog['kmeans_cluster'] == cluster].shape[0] / cluster_homog.shape[0] * 100\n",
    "\n",
    "    sorted_clusters = sorted(cluster_freq, key = cluster_freq.get, reverse = True)\n",
    "    l_sorted_clusters = []\n",
    "    for r in sorted_clusters:\n",
    "        l_sorted_clusters.append([r, cluster_freq[r]])\n",
    "    df_sorted_clusters = pd.DataFrame(l_sorted_clusters, columns = ['cluster_id', 'Percent of Total'])\n",
    "\n",
    "    plt.figure(figsize = (5,5))\n",
    "    ax = sns.barplot(data = df_sorted_clusters, x = 'cluster_id', y = 'Percent of Total', palette = 'tab20')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.title('Percent of Total Cell Count per Neighborhood Cluster')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(neighbor_plots_dir + '/Per_Slide/{}_clusters_fractions.pdf'.format(current_slide), dpi=500)\n",
    "    plt.close()                                                                                                                  # UPDATED ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel processing of all slides.  Can be time consuming depending on size of dataset \n",
    "Parallel(n_jobs = int(available_cores * 0.9), backend = 'loky')(delayed(neighborSlide)(x)for x in tqdm([slide_id]))            # For single, multiple slides unbracket 'slide_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Voronoi Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter row, col, and slide names for tile of interest\n",
    "vor_row = '0'\n",
    "vor_col = '1'\n",
    "vor_slide = 'N16-244-2A'\n",
    "vor_tile = df_full_dataset[(df_full_dataset['Tile']=='({}, {})'.format(vor_row, vor_col)) & (df_full_dataset['slide_id']==vor_slide)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.asarray(vor_tile[['centroid-1', 'centroid-0']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vor = Voronoi(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = voronoi_plot_2d(vor, show_vertices = False, show_points = False)\n",
    "fig.set_size_inches(15,15)\n",
    "for l in l_pops:\n",
    "    for j in range(len(coords)):\n",
    "        region = vor.regions[vor.point_region[j]]\n",
    "        if not -1 in region:\n",
    "            polygon = [vor.vertices[i] for i in region]\n",
    "            if vor_tile['pop_ID'].iloc[j] == l:\n",
    "                color = vor_pops[l]\n",
    "                plt.fill(*zip(*polygon), color)\n",
    "plt.ylim(0,tile_y)\n",
    "plt.xlim(0,tile_x)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "handles = []\n",
    "for index, x in enumerate(l_pops):\n",
    "    h = mpatches.Patch(color = vor_pops[l_pops[index]], label = l_pops[index])\n",
    "    handles.append(h)\n",
    "    \n",
    "plt.legend(handles = handles)\n",
    "\n",
    "plt.savefig(neighborhood_dir + '/Voronoi_{}_r{}_c{}.pdf'.format(vor_slide, vor_row, vor_col), dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample niche images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_clustered = glob(neighbor_umap_dir + '/*.csv')\n",
    "for index, l in enumerate(l_clustered):\n",
    "    print(str(index) + ' : ' +  os.path.basename(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter index number of slide of interest\n",
    "slide_of_interest = 0\n",
    "df_sample_plots = pd.read_csv(l_clustered[slide_of_interest])\n",
    "sample_slide = os.path.basename(l_clustered[slide_of_interest]).split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for each cluster, selects 3 index cells at random and prints voronoi plots colored by population IDs\n",
    "#sets radius of window around sample index cells\n",
    "radius = 50\n",
    "\n",
    "for index, row in df_sample_plots.groupby('kmeans_cluster').sample(3).iterrows():\n",
    "    cell_y = row['centroid-0']\n",
    "    cell_x = row['centroid-1']\n",
    "    pat = row['slide_id']\n",
    "    tile_r = row['Tile_row']\n",
    "    tile_c = row['Tile_col']\n",
    "    cluster = row['kmeans_cluster']\n",
    "    \n",
    "    vor_tile = df_sample_plots[(df_sample_plots['Tile_row'] == tile_r) & (df_sample_plots['Tile_col'] == tile_c)]\n",
    "    coords = np.asarray(vor_tile[['centroid-1', 'centroid-0']])    \n",
    "    vor = Voronoi(coords)\n",
    "    \n",
    "    fig = voronoi_plot_2d(vor, show_vertices = False, show_points = False)\n",
    "    fig.set_size_inches(10,10)\n",
    "    for l in l_pops:\n",
    "        for j in range(len(coords)):\n",
    "            region = vor.regions[vor.point_region[j]]\n",
    "            if not -1 in region:\n",
    "                polygon = [vor.vertices[i] for i in region]\n",
    "                if vor_tile['pop_ID'].iloc[j] == l:\n",
    "                    color = vor_pops[l]\n",
    "                    plt.fill(*zip(*polygon), color)\n",
    "    plt.ylim(cell_y-radius,cell_y+radius)\n",
    "    plt.xlim(cell_x -radius,cell_x + radius)\n",
    "    plt.plot(cell_x, cell_y, 'ro')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    handles = []\n",
    "    for i, x in enumerate(l_pops):\n",
    "        h = mpatches.Patch(color = vor_pops[l_pops[i]], label = l_pops[i])\n",
    "        handles.append(h)\n",
    "\n",
    "    plt.legend(handles = handles)\n",
    "\n",
    "    plt.savefig(neighbor_plots_dir + '/Per_Cluster/Vor_cluster{}_{}.pdf'.format(cluster, index), dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slide colored by clusterID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot each cluster by itself per slide\n",
    "for slide in l_clustered:\n",
    "    df_sample_plots = pd.read_csv(slide)\n",
    "    sample_slide = os.path.basename(slide).split('_')[0]\n",
    "    for l in range(len(df_sample_plots['kmeans_cluster'].unique())):\n",
    "        plt.figure(figsize = (10,10))\n",
    "        sns.scatterplot(data = df_sample_plots, x = 'Global_x', y = 'Global_y', color = 'gray', alpha = 0.25, linewidth = 0, s = 5).set_aspect('equal')\n",
    "        sns.scatterplot(data = df_sample_plots[df_sample_plots['kmeans_cluster']==l], x = 'Global_x', y = 'Global_y', color = 'blue', linewidth = 0, s = 5).set_aspect('equal')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(neighbor_plots_dir + '/Per_Slide/{}_Neighborhood_Cluster_{}.pdf'.format(sample_slide, l), dpi=500)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot all points colored by cluster per slide\n",
    "for slide in l_clustered:\n",
    "    df_sample_plots = pd.read_csv(slide)\n",
    "    sample_slide = os.path.basename(slide).split('_')[0]\n",
    "    \n",
    "    plt.figure(figsize = (20,20))\n",
    "    sns.scatterplot(data = df_sample_plots, x = 'Global_x', y = 'Global_y', hue = 'kmeans_cluster', palette = 'tab20', linewidth=0, s = 4)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.savefig(neighbor_plots_dir + '/Per_Slide/{}_All_Clusters_Colored_largedots.pdf'.format(sample_slide, l), dpi=500)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all points colored by cluster per slide\n",
    "for slide in l_clustered:\n",
    "    df_sample_plots = pd.read_csv(slide)\n",
    "    sample_slide = os.path.basename(slide).split('_')[0]\n",
    "    \n",
    "    plt.figure(figsize = (20,20))\n",
    "    sns.scatterplot(data = df_sample_plots, x = 'Global_x', y = 'Global_y', hue = 'kmeans_cluster', palette = 'tab20', linewidth=0, s = 1)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.savefig(neighbor_plots_dir + '/Per_Slide/{}_All_Clusters_Colored_smalldots.pdf'.format(sample_slide, l), dpi=500)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all points colored by gated population ID per slide\n",
    "for slide in l_clustered:\n",
    "    df_sample_plots = pd.read_csv(slide)\n",
    "    sample_slide = os.path.basename(slide).split('_')[0]\n",
    "    \n",
    "    plt.figure(figsize = (20,20))\n",
    "    sns.scatterplot(data = df_sample_plots, x = 'Global_x', y = 'Global_y', hue = 'pop_ID', palette = vor_pops, linewidth=0, s = 4)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.savefig(neighbor_plots_dir + '/Per_Slide/{}_All_Pops_Colored_s4.pdf'.format(sample_slide, l), dpi=500)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all points colored by gated population ID per slide\n",
    "for slide in l_clustered:\n",
    "    df_sample_plots = pd.read_csv(slide)\n",
    "    sample_slide = os.path.basename(slide).split('_')[0]\n",
    "    \n",
    "    plt.figure(figsize = (20,20))\n",
    "    sns.scatterplot(data = df_sample_plots, x = 'Global_x', y = 'Global_y', hue = 'pop_ID', palette = vor_pops, linewidth=0, s = 1)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.savefig(neighbor_plots_dir + '/Per_Slide/{}_All_Pops_Colored_s1.pdf'.format(sample_slide, l), dpi=500)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decagon Drawings of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for drawing in range(len(np.unique(df_sample_plots['kmeans_cluster']))):\n",
    "    rounded = df_sample_plots[l_pops][df_sample_plots['kmeans_cluster'] == drawing].mean()                    # UPDATED\n",
    "    rounded = pd.DataFrame(rounded)                                                                     ####CHECK WHY [1:] is included!\n",
    "    \n",
    "    rounded['dec'] = rounded.apply(lambda x: np.floor(x*10))\n",
    "    rounded['rem'] = rounded[0].apply(lambda x: (x*10)-int(x*10))\n",
    "    tot = 10 - int(rounded['dec'].sum())\n",
    "    rounded.nlargest(tot, ['rem'])['dec'].apply(lambda x : x + 1)\n",
    "    rounded.update(pd.DataFrame(rounded.nlargest(tot, ['rem'])['dec'].apply(lambda x : x + 1)))\n",
    "    rounded['dec'] = rounded['dec'].apply(np.int64)\n",
    "    rounded = rounded.sort_values(by = ['dec'], ascending = False)\n",
    "    for_dec = rounded[rounded.dec != 0].reset_index()\n",
    "    \n",
    "    d_dec = {}\n",
    "    for index, row in for_dec.iterrows():\n",
    "        d_dec[row['index']] = row['dec']\n",
    "    d_turtle = {}\n",
    "    count2 = 0\n",
    "    for key, val in d_dec.items():\n",
    "        count = val\n",
    "        while count > 0:\n",
    "            d_turtle[count2] = vor_pops[key]\n",
    "            count -= 1\n",
    "            count2 +=1\n",
    "        else:\n",
    "            count = 0\n",
    "            \n",
    "    pen = 6\n",
    "    side = 100\n",
    "\n",
    "    t = turtle.Turtle()\n",
    "    t.hideturtle()\n",
    "    t.pensize(pen)\n",
    "    t.speed(0)\n",
    "    for i in range(10):\n",
    "        t.forward(side)\n",
    "        t.right(36)\n",
    "    t.penup()\n",
    "    t.goto(0, side*3+pen)\n",
    "    t.pendown()\n",
    "    for y in range(10):\n",
    "        t.fillcolor(d_turtle[y])\n",
    "        if y == 0:\n",
    "            t.begin_fill()\n",
    "            for i in range(10):\n",
    "                t.forward(side)\n",
    "                t.right(36)\n",
    "            t.end_fill()\n",
    "            t.penup()\n",
    "            for k in range(2):\n",
    "                t.forward(side)\n",
    "                t.right(36)\n",
    "            t.left(72)\n",
    "            t.pendown()\n",
    "        else:\n",
    "            t.fillcolor(d_turtle[y])\n",
    "            t.begin_fill()\n",
    "            for i in range(10):\n",
    "                t.forward(side)\n",
    "                t.right(36)\n",
    "            t.end_fill()\n",
    "            t.penup()\n",
    "            for k in range(3):\n",
    "                t.forward(side)\n",
    "                t.right(36)\n",
    "            t.left(72)\n",
    "            t.pendown()  \n",
    "    ts = t.getscreen().getcanvas().postscript(file = 'test.eps', width = 1500, height = 1500)\n",
    "    img = Image.open('test.eps')\n",
    "    img.save(neighbor_plots_dir + '/cluster{}.jpg'.format(drawing))\n",
    "    t.clear()\n",
    "turtle.bye()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neighborhood Analysis on All or Multiple Slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use if cell neighborhoods spanning multiple slides are desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all niche csvs.  Add global-x, global-y coordinates\n",
    "l_niche = []\n",
    "for root, dirs, files in os.walk(neighbor_csv_dir):\n",
    "    for i in files:\n",
    "        if i.endswith('.csv'):\n",
    "            l_niche.append(os.path.join(root, i))\n",
    "l_niche = sorted(l_niche)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Full Dataset neighborhood and feature data\n",
    "frames = []\n",
    "for i in l_niche:\n",
    "    k = pd.read_csv(i)\n",
    "    frames.append(k)\n",
    "\n",
    "df_full_dataset = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l_slides)\n",
    "l_slide_subset = ['test', 'text', 'for', 'example']   # Insert slide_id text strings for each slide to include in grouped neighborhood analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset_dataset = df_full_dataset.loc[(df_full_dataset['slide_id'].isin(l_slide_subset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample all slides equally to a total of 100000 events\n",
    "l_actual = sorted(df_subset_dataset['slide_id'].unique())\n",
    "goal_sample = int(100000/len(l_actual))\n",
    "\n",
    "l_lens = []\n",
    "for slide in l_actual:\n",
    "    l_lens.append([slide, len(df_subset_dataset[df_subset_dataset['slide_id']==slide])])\n",
    "    \n",
    "if min(x[1] for x in l_lens) < goal_sample:\n",
    "    goal_sample = min(x[1] for x in l_lens)\n",
    "    \n",
    "l_sampled = []\n",
    "for k in l_lens:\n",
    "    df_k = df_subset_dataset[df_subset_dataset['slide_id']==k[0]]\n",
    "    df_k_k = df_k.sample(n = goal_sample)\n",
    "    l_sampled.append(df_k_k)\n",
    "df_sampled = pd.concat(l_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot abundance of each gated population\n",
    "pop_freq = {}\n",
    "for pop in l_pops:\n",
    "    pop_freq[pop] = df_sampled[pop].sum() / df_sampled.shape[0]*100\n",
    "\n",
    "sorted_pops = sorted(pop_freq, key = pop_freq.get, reverse = True)\n",
    "l_sorted_pops = []\n",
    "for r in sorted_pops:\n",
    "    l_sorted_pops.append([r, pop_freq[r]])\n",
    "df_sorted_pops = pd.DataFrame(l_sorted_pops, columns = ['pop_id', 'Percent of Total'])\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "ax = sns.barplot(data = df_sorted_pops, x = 'pop_id', y = 'Percent of Total', palette = vor_pops)\n",
    "plt.xticks(rotation = 90)\n",
    "ax.set_yscale('log')\n",
    "plt.ylim([0,100])\n",
    "plt.title('Fraction of Total Cell Count per Population')\n",
    "plt.tight_layout()\n",
    "plt.savefig(neighbor_plots_dir + f'/MergedSlides/{str_now}_all_slides_populations_fractions.pdf', dpi=500)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run UMAP on downsampled dataset, followed by upsampling to add remaining cells into the embedding\n",
    "drop_cols = l_pops\n",
    "df_embed = df_sampled.drop(df_sampled.columns.difference(drop_cols), 1)\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors = 50, min_dist = 0.1, random_state = seed)\n",
    "\n",
    "embedding = reducer.fit_transform(df_embed)\n",
    "\n",
    "df_embedding = pd.DataFrame(data = embedding, columns = ['umap1', 'umap2'])\n",
    "\n",
    "df_sampled_final = df_sampled.reset_index(drop = True).join(df_embedding)\n",
    "\n",
    "drop_cols = l_pops + ['centroid.0', 'centroid.1']\n",
    "upsampled_embedding = reducer.transform(df_subset_dataset.drop(df_subset_dataset.columns.difference(drop_cols),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find neighbor cluster id for all cells \n",
    "df_upsampled_embedding = pd.DataFrame(data = upsampled_embedding, columns = ['umap1', 'umap2'])\n",
    "df_upsampled_final = df_subset_dataset.reset_index(drop = True).join(df_upsampled_embedding)\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters = 15, random_state = seed)\n",
    "kmeans.fit(df_upsampled_embedding)\n",
    "\n",
    "y_upsampled_kmeans = kmeans.predict(df_upsampled_embedding)\n",
    "df_upsampled_kmeans = pd.DataFrame(y_upsampled_kmeans, columns = ['kmeans_cluster'])\n",
    "df_upsampled_final = df_upsampled_final.join(df_upsampled_kmeans)\n",
    "df_upsampled_final.to_csv(neighbor_plots_dir + f'/MergedSlides/{str_now}_upsampled_umap.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "ax = sns.scatterplot(data = df_upsampled_final, x = 'umap1', y = 'umap2', hue = 'kmeans_cluster', s = 3, linewidth = 0, palette = 'tab20')\n",
    "ax.set_aspect('equal')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc = 2, borderaxespad = 0.)\n",
    "plt.tight_layout()\n",
    "plt.savefig(neighbor_plots_dir + f'/MergedSlides/{str_now}_clusters_on_umap.pdf', dpi=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_cols = list(pop_freq.keys()).copy()\n",
    "cluster_cols.append('kmeans_cluster')\n",
    "n_clusters = 15\n",
    "\n",
    "\n",
    "df_clusters = df_upsampled_final.drop(df_upsampled_final.columns.difference(cluster_cols), 1)\n",
    "\n",
    "df_clusters = df_clusters.groupby('kmeans_cluster').mean()  \n",
    "\n",
    "sns.clustermap(df_clusters, cmap = 'viridis')\n",
    "plt.tight_layout()\n",
    "plt.savefig(neighbor_plots_dir + '/MergedSlides/merged_clustergram.pdf', dpi=500)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_cols = cluster_cols.copy()\n",
    "drop_cols.extend(['centroid.0', 'centroid.1'])\n",
    "upsampled_cluster_homog = df_upsampled_final[['centroid.0', 'centroid.1', 'kmeans_cluster']].merge(df_subset_dataset.drop(df_single.columns.difference(drop_cols),1), on =['centroid.0', 'centroid.1'])\n",
    "\n",
    "upsampled_cluster_homog = upsampled_cluster_homog.drop(columns = ['centroid.0', 'centroid.1'])\n",
    "\n",
    "df_upsampled_homog = pd.DataFrame()\n",
    "for i in range(len(np.unique(y_upsampled_kmeans))):\n",
    "    out = pd.melt(upsampled_cluster_homog[upsampled_cluster_homog['kmeans_cluster']==i].drop(columns = 'kmeans_cluster')).groupby('variable').quantile(0.90).rename(columns = {'value' : 'kmeans_cluster_{}'.format(i)}).reset_index()\n",
    "    if i == 0:\n",
    "        df_upsampled_homog = out\n",
    "    else:\n",
    "        df_upsampled_homog = pd.merge(df_upsampled_homog, out, on = 'variable')\n",
    "\n",
    "order = df_upsampled_homog.loc[(df_upsampled_homog.iloc[:, 1:]!=0).any(axis = 1)]['variable'].tolist()[::-1]\n",
    "\n",
    "fig, axs = plt.subplots(int(n_clusters/4)+1,4, figsize = (20, int((n_clusters/4+1))*5))\n",
    "fig.subplots_adjust(hspace=1,wspace=0.4)\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
    "\n",
    "\n",
    "for i in range(len(np.unique(y_upsampled_kmeans))):\n",
    "    plt.subplot(4,int(n_clusters/4)+1, i+1)\n",
    "    g = sns.boxplot(data = pd.melt(upsampled_cluster_homog[upsampled_cluster_homog['kmeans_cluster']==i].drop(columns = 'kmeans_cluster')), x = 'variable', y = 'value', showfliers = False, order = order, palette = vor_pops)\n",
    "    g.set(ylim=(0,1.2))\n",
    "    g.tick_params(axis = 'x', labelrotation = 90)\n",
    "    g.set_xlabel('')\n",
    "    g.set_ylabel('Neighbor Frequency')\n",
    "    g.set_title('Cluster {}, n = {}'.format(str(i), len(upsampled_cluster_homog[upsampled_cluster_homog['kmeans_cluster']==i])))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(neighbor_plots_dir + f'/MergedSlides/{str_now}_upsampled_homogeniety.pdf', dpi=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_freq = {}                                                                                                           \n",
    "for cluster in sorted(upsampled_cluster_homog['kmeans_cluster'].unique()):\n",
    "    cluster_freq[cluster] = upsampled_cluster_homog[upsampled_cluster_homog['kmeans_cluster'] == cluster].shape[0] / upsampled_cluster_homog.shape[0] * 100\n",
    "\n",
    "sorted_clusters = sorted(cluster_freq, key = cluster_freq.get, reverse = True)\n",
    "l_sorted_clusters = []\n",
    "for r in sorted_clusters:\n",
    "    l_sorted_clusters.append([r, cluster_freq[r]])\n",
    "df_sorted_clusters = pd.DataFrame(l_sorted_clusters, columns = ['cluster_id', 'Percent of Total'])\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "ax = sns.barplot(data = df_sorted_clusters, x = 'cluster_id', y = 'Percent of Total', palette = 'tab20')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title('Percent of Total Cell Count per Neighborhood Cluster')\n",
    "plt.tight_layout()\n",
    "plt.savefig(neighbor_plots_dir + f'/MergedSlides/{str_now}_clusters_fractions.pdf', dpi=500)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot all points colored by cluster per slide\n",
    "for slide in l_slides:\n",
    "    df_sample_plots = df_upsampled_final[df_upsampled_final['slide_id']==slide]\n",
    "    \n",
    "    plt.figure(figsize = (20,20))\n",
    "    sns.scatterplot(data = df_sample_plots, x = 'Global_x', y = 'Global_y', hue = 'kmeans_cluster', palette = 'tab20', linewidth=0, s = 4)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.savefig(neighbor_plots_dir + f'/MergedSlides/{str_now}_{slide}_All_Clusters_Colored_largedots.pdf', dpi=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot all points colored by cluster per slide\n",
    "for slide in l_slides:\n",
    "    df_sample_plots = df_upsampled_final[df_upsampled_final['slide_id']==slide]\n",
    "    \n",
    "    plt.figure(figsize = (20,20))\n",
    "    sns.scatterplot(data = df_sample_plots, x = 'Global_x', y = 'Global_y', hue = 'kmeans_cluster', palette = 'tab20', linewidth=0, s = 1)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.savefig(neighbor_plots_dir + f'/MergedSlides/{str_now}_{slide}_All_Clusters_Colored_smalldots.pdf', dpi=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot all points colored by gated population ID per slide\n",
    "for slide in l_slides:\n",
    "    df_sample_plots = df_upsampled_final[df_upsampled_final['slide_id']==slide]\n",
    "    \n",
    "    plt.figure(figsize = (20,20))\n",
    "    sns.scatterplot(data = df_sample_plots, x = 'Global_x', y = 'Global_y', hue = 'pop_ID', palette = vor_pops, linewidth=0, s = 4)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.savefig(neighbor_plots_dir + f'/MergedSlides/{str_now}_{slide}_All_Pops_Colored_s4.pdf', dpi=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot all points colored by gated population ID per slide\n",
    "for slide in l_slides:\n",
    "    df_sample_plots = df_upsampled_final[df_upsampled_final['slide_id']==slide]\n",
    "    \n",
    "    plt.figure(figsize = (20,20))\n",
    "    sns.scatterplot(data = df_sample_plots, x = 'Global_x', y = 'Global_y', hue = 'pop_ID', palette = vor_pops, linewidth=0, s = 1)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.savefig(neighbor_plots_dir + f'/MergedSlides/{str_now}_{slide}_All_Pops_Colored_s1.pdf', dpi=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decagon Drawings of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for drawing in range(len(np.unique(df_upsampled_final['kmeans_cluster']))):\n",
    "    rounded = df_upsampled_final[l_pops][df_upsampled_final['kmeans_cluster'] == drawing].mean()                    # UPDATED\n",
    "    rounded = pd.DataFrame(rounded[1:], columns = ['frac'])\n",
    "    \n",
    "    rounded['dec'] = rounded.apply(lambda x: np.floor(x*10))\n",
    "    rounded['rem'] = rounded['frac'].apply(lambda x: (x*10)-int(x*10))\n",
    "    tot = 10 - int(rounded['dec'].sum())\n",
    "    rounded.nlargest(tot, ['rem'])['dec'].apply(lambda x : x + 1)\n",
    "    rounded.update(pd.DataFrame(rounded.nlargest(tot, ['rem'])['dec'].apply(lambda x : x + 1)))\n",
    "    rounded['dec'] = rounded['dec'].apply(np.int64)\n",
    "    rounded = rounded.sort_values(by = ['dec'], ascending = False)\n",
    "    for_dec = rounded[rounded.dec != 0].reset_index()\n",
    "    \n",
    "    d_dec = {}\n",
    "    for index, row in for_dec.iterrows():\n",
    "        d_dec[row['index']] = row['dec']\n",
    "    d_turtle = {}\n",
    "    count2 = 0\n",
    "    for key, val in d_dec.items():\n",
    "        count = val\n",
    "        while count > 0:\n",
    "            d_turtle[count2] = vor_pops[key]\n",
    "            count -= 1\n",
    "            count2 +=1\n",
    "        else:\n",
    "            count = 0\n",
    "            \n",
    "    pen = 6\n",
    "    side = 100\n",
    "\n",
    "    t = turtle.Turtle()\n",
    "    t.hideturtle()\n",
    "    t.pensize(pen)\n",
    "    t.speed(0)\n",
    "    for i in range(10):\n",
    "        t.forward(side)\n",
    "        t.right(36)\n",
    "    t.penup()\n",
    "    t.goto(0, side*3+pen)\n",
    "    t.pendown()\n",
    "    for y in range(10):\n",
    "        t.fillcolor(d_turtle[y])\n",
    "        if y == 0:\n",
    "            t.begin_fill()\n",
    "            for i in range(10):\n",
    "                t.forward(side)\n",
    "                t.right(36)\n",
    "            t.end_fill()\n",
    "            t.penup()\n",
    "            for k in range(2):\n",
    "                t.forward(side)\n",
    "                t.right(36)\n",
    "            t.left(72)\n",
    "            t.pendown()\n",
    "        else:\n",
    "            t.fillcolor(d_turtle[y])\n",
    "            t.begin_fill()\n",
    "            for i in range(10):\n",
    "                t.forward(side)\n",
    "                t.right(36)\n",
    "            t.end_fill()\n",
    "            t.penup()\n",
    "            for k in range(3):\n",
    "                t.forward(side)\n",
    "                t.right(36)\n",
    "            t.left(72)\n",
    "            t.pendown()  \n",
    "    ts = t.getscreen().getcanvas().postscript(file = 'test.eps', width = 1500, height = 1500)\n",
    "    img = Image.open('test.eps')\n",
    "    img.save(neighbor_plots_dir + f'/MergedSlides/{str_now}_upsampled_cluster{drawing}.jpg')\n",
    "    t.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
